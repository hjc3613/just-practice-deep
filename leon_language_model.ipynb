{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "leon-language-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/just-practice-deep/blob/master/leon_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIEvNA751Skl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S0aImLV10ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM(nn.Module):\n",
        "  \n",
        "  def __init__(self, hid_size, vocab_size, n_head, n_layers, max_len, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "    \n",
        "    self.hid_size = hid_size\n",
        "    self.max_len = max_len\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, hid_size)\n",
        "\n",
        "    self.position_enc = nn.Embedding(self.max_len, self.hid_size)\n",
        "    self.position_enc.weight.data = self.position_encoding_init(self.max_len, self.hid_size)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.hid_size])).to(device)\n",
        "\n",
        "    self.decoder_layer = nn.TransformerDecoderLayer(d_model=hid_size, nhead = n_head)\n",
        "    self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=n_layers)\n",
        "    self.fc = nn.Linear(hid_size, vocab_size)\n",
        "\n",
        "    self._init_weights()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    sent_len, batch_size = x.shape[0], x.shape[1]\n",
        "    memory_mask = self.generate_complete_mask(sent_len)\n",
        "    tgt_mask = self.generate_triangular_mask(sent_len)\n",
        "    memory = torch.randn(sent_len, batch_size, self.hid_size, device=self.device)\n",
        "\n",
        "    temp = x\n",
        "    temp = self.embedding(temp)\n",
        "\n",
        "    pos = torch.arange(0,sent_len).unsqueeze(1).repeat(1,batch_size).to(self.device)\n",
        "    temp_pos_emb = self.position_enc(pos)\n",
        "\n",
        "    temp = temp * self.scale + temp_pos_emb\n",
        "    temp = self.decoder(temp, memory, tgt_mask=tgt_mask)\n",
        "    temp = self.fc(temp)\n",
        "    return temp\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "  def generate_triangular_mask(self, size):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
        "        return\n",
        "        \n",
        "  def generate_complete_mask(self, size):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = torch.empty(size, size).to(device)\n",
        "        mask.fill_(float('-inf'))\n",
        "        return mask\n",
        "\n",
        "  def generate_sequence(self, src):\n",
        "    #src = [sent_len]\n",
        "    src = src.unsqueeze(1)\n",
        "    #src = [sent_len, 1]\n",
        "    generate_step = 0\n",
        "    while generate_step < 10:\n",
        "      out = self.forward(src)\n",
        "      #out = [sent_len + 1, 1, vocab_size]\n",
        "      out = torch.argmax(out[-1, :], dim=1) # [1]\n",
        "      out = out.unsqueeze(0) #[1,1]\n",
        "      src = torch.cat((src, out), dim=0)\n",
        "      generate_step += 1\n",
        "    src = src.squeeze(1)\n",
        "    return src\n",
        "  \n",
        "  def position_encoding_init(self, n_position, d_pos_vec):\n",
        "    ''' Init the sinusoid position encoding table '''\n",
        "\n",
        "    # keep dim 0 for padding token position encoding zero vector\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
        "    temp = torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
        "    temp = temp.to(self.device)\n",
        "    return temp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1auimPW-3v5i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "546e8b76-4d73-4b9c-83de-617333810a03"
      },
      "source": [
        "sent_len = 20\n",
        "batch_size = 64\n",
        "vocab_size = 1000\n",
        "hid_dim = 512\n",
        "x = torch.LongTensor(20, 64).random_(1000).to(device)\n",
        "lm = LM(512, 1000, 8, 6, 1024,device).to(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6c289015af15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhid_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-ada39b6c2e0a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hid_size, vocab_size, n_head, n_layers, max_len, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_encoding_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'position_encoding_init' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmPisVZm4fAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm(x).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSP_8TBqDhX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(lm(x)[0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PAM7eez9A_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext import data\n",
        "import spacy\n",
        " \n",
        "my_tok = spacy.load('en')\n",
        " \n",
        "def spacy_tok(x):\n",
        "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
        " \n",
        "TEXT = data.Field(lower=True, tokenize=spacy_tok)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFmEHplq9RY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.datasets import WikiText2\n",
        " \n",
        "train, valid, test = WikiText2.splits(TEXT) # loading custom datasets requires passing in the field, but nothing else."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m0vLlNj9U9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train)\n",
        "print(len(TEXT.vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y7BmeNw9Y80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_size=128,\n",
        "    bptt_len=8, # this is where we specify the sequence length\n",
        "    device=device,\n",
        "    repeat=False)\n",
        "\n",
        "print(len(train_iter))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2jNIDm99aqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = next(iter(train_iter))\n",
        "b.__dict__.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kz4kJXOCKj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "hid_size = 512\n",
        "model = LM(hid_size, vocab_size, 8, 6, 1024, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuKNVTcKGu4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8AdJMAb9pF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = NoamOpt(hid_size, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwK2wn_k-peb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "clip = 0.5\n",
        "\n",
        "N_EPOCH = 5\n",
        "for epoch in range(N_EPOCH):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  for batch in tqdm(train_iter):\n",
        "    optimizer.zero_grad()\n",
        "    batch_text = batch.text\n",
        "    batch_target = batch.target\n",
        "    result = model(batch_text)\n",
        "    loss = criterion(result.view(-1, result.shape[-1]), batch_target.view(-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  print(\"\\n\", epoch_loss / len(train_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPSAlyzdyfem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_sentence = [\"i\",\"listen\"]\n",
        "model.eval()\n",
        "print(' '.join(source_sentence))\n",
        "print()\n",
        "x = TEXT.numericalize([source_sentence]).to(device).squeeze(1)\n",
        "generated_sequence =model.generate_sequence(x)\n",
        "words = [TEXT.vocab.itos[word_idx] for word_idx in generated_sequence]\n",
        "print(' '.join(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGYWmunUxGHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = torch.arange(0,100).unsqueeze(1).repeat(1,64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVrAeB5p2bxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}