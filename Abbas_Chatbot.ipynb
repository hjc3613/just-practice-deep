{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bradley_Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/just-practice-deep/blob/master/Abbas_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45uNP7CfX7Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data import Field\n",
        "import spacy\n",
        "\n",
        "from spacy.symbols import ORTH\n",
        "my_tok = spacy.load('en')\n",
        "\n",
        "def spacy_tok(x):\n",
        "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
        "\n",
        "QUERY = Field(lower=True, tokenize=spacy_tok)\n",
        "RESPONSE = Field(lower=True, tokenize=spacy_tok, is_target=True, init_token='<bos>', eos_token='<eos>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gmmeZHvZl73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available:\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8HlpeMbZnz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"n't\"}])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dy7TSTGZpoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset\n",
        "\n",
        "train_dataset = TabularDataset(path=\"./formatted_movie_lines.txt\", format=\"CSV\",\n",
        "                               fields=[(\"query\", QUERY),(\"response\", RESPONSE)],\n",
        "                               csv_reader_params={\"delimiter\":'\\t'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv-wQRDlNvgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "fcadae34-ef54-450a-f348-1e1f3db020be"
      },
      "source": [
        "QUERY.build_vocab(train_dataset)\n",
        "RESPONSE.build_vocab(train_dataset)\n",
        "print(QUERY.vocab.stoi['film'])\n",
        "print(QUERY.vocab.itos[33])\n",
        "print(len(QUERY.vocab))\n",
        "print(len(RESPONSE.vocab))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1059\n",
            "'re\n",
            "48505\n",
            "49036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEjDykrEabVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import BucketIterator, interleave_keys\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    # (new example to add, current effective batch size, current count of examples in the batch) \n",
        "    # when returned value meets batch_size (effective, innate effective batch_size \n",
        "    # defined as global bala bala) then wraper create a batch\n",
        "\n",
        "    sum_len = len(new.query) + len(new.response)\n",
        "\n",
        "    if sum_len > 500:\n",
        "      return batch_size\n",
        "    elif sum_len > 300: \n",
        "      return sofar + 16\n",
        "    elif sum_len > 200: \n",
        "      return sofar + 8\n",
        "    elif sum_len > 120: \n",
        "      return sofar + 4\n",
        "    elif sum_len > 60: \n",
        "      return sofar + 2\n",
        "    elif sum_len > 30: \n",
        "      return sofar + 1\n",
        "    elif sum_len > 20: \n",
        "      return sofar + 0.5\n",
        "    else:\n",
        "      return sofar + 0.25\n",
        "    \n",
        "    \n",
        "       \n",
        "\n",
        "train_iterator = BucketIterator(dataset= train_dataset, batch_size=batch_size,\n",
        "                                batch_size_fn = batch_size_fn\n",
        "                                ,device=device\n",
        "                                ,sort_key=lambda x: interleave_keys(len(x.query), len(x.response))\n",
        "                                , sort = True\n",
        "                                , repeat = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQvdKHxvCZjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "e6588b1c-7475-41ad-bdda-c9837682ffb9"
      },
      "source": [
        "## test if data loads well?\n",
        "a = list(iter(train_iterator))\n",
        "q = a[20].query[:,0]\n",
        "r = a[20].response[:,0]\n",
        "for v in q:\n",
        "  print(QUERY.vocab.itos[v])\n",
        "print(\"**********\")\n",
        "for v in r:\n",
        "  print(RESPONSE.vocab.itos[v])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "his\n",
            "mother\n",
            "?\n",
            "**********\n",
            "<bos>\n",
            "what\n",
            "?\n",
            "<eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4iHd1PvadmJ",
        "colab_type": "code",
        "outputId": "08799cea-ad2a-473f-dd9e-9bb85c5da73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_batch = 0\n",
        "for batch in (iter(train_iterator)):\n",
        "  #print(batch.query.shape, \"\\t\", batch.response.shape)\n",
        "  num_batch += 1\n",
        "print(\"number of batch is:\", num_batch)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of batch is: 4336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x8xba4xayJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class Bradley(nn.Module):\n",
        "  def __init__(self, src_voc_sze, trg_voc_sze, hid_sze, num_head, num_enc, num_dec):\n",
        "    super(Bradley, self).__init__()\n",
        "    self.hid_sze = hid_sze\n",
        "    self.src_word_embedding = nn.Embedding(src_voc_sze, self.hid_sze)\n",
        "    self.trg_word_embedding = nn.Embedding(trg_voc_sze, self.hid_sze)\n",
        "    self.trg_pos_embedding = nn.Embedding(800, self.hid_sze)\n",
        "    self.num_head = num_head\n",
        "    self.transformer = nn.Transformer(self.hid_sze, self.num_head, num_enc, num_dec)\n",
        "    self.fc = nn.Linear(self.hid_sze, trg_voc_sze)\n",
        "  \n",
        "  def forward(self, src, trg):\n",
        "    #src = [src sent len, batch_size]\n",
        "    #trg = [trg sent len, batch_size]\n",
        "    temp_src = self.src_word_embedding(src)\n",
        "    temp_trg = self.trg_word_embedding(trg)\n",
        "    trg_sent_len, batch_size = trg.shape[0], trg.shape[1]\n",
        "    trg_pos = self.trg_pos_embedding(torch.arange(0, trg_sent_len).unsqueeze(0).\n",
        "                                     repeat(batch_size,1).to(device)).transpose(0,1)\n",
        "    trg_mask = self._generate_square_subsequent_mask(trg_sent_len)\n",
        "    temp_trg = temp_trg + trg_pos\n",
        "    temp = self.transformer(temp_src, temp_trg, tgt_mask=trg_mask)\n",
        "    return self.fc(temp)\n",
        "  \n",
        "  def _generate_square_subsequent_mask(self, sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    mask = mask.to(device)\n",
        "    return mask\n",
        "\n",
        "\n",
        "  def greedy_inference_one_sample(self, src, max_len=20):\n",
        "\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      #src = [src sent len]\n",
        "      src_len = src.shape[0]\n",
        "      src = src.unsqueeze(1)\n",
        "      #src = [sent_len, 1]\n",
        "      trg = src.new_full((1,1), RESPONSE.vocab.stoi['<bos>'])\n",
        "      #trg = [1,1]\n",
        "\n",
        "      translation_step = 0\n",
        "      while translation_step < max_len:\n",
        "        out = self.forward(src, trg)\n",
        "        out = out[-1,:]\n",
        "        #out = [batch_size, trg_vocab_size]\n",
        "        nex = out.argmax(dim=1).unsqueeze(0)\n",
        "        #nex = [1, 1]\n",
        "        trg = torch.cat((trg, nex), dim=0)\n",
        "        translation_step += 1\n",
        "    return trg\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QgtTbBmcsNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bradley_model = Bradley(src_voc_sze=len(QUERY.vocab), trg_voc_sze=len(RESPONSE.vocab),\n",
        "                        hid_sze=64, num_head=4,\n",
        "                        num_enc=4, num_dec=3)\n",
        "bradley_model = bradley_model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr8hvOmTeGUy",
        "colab_type": "code",
        "outputId": "1eebe0b8-038b-4f58-bb42-c3611497ce46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=RESPONSE.vocab.stoi['<pad>'])\n",
        "lr = 5\n",
        "optimizer = torch.optim.SGD(bradley_model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "epoch_number = 1\n",
        "\n",
        "print(len(train_dataset)/batch_size)\n",
        "\n",
        "for epoch in range(1, epoch_number+1):\n",
        "  print(\"epoch \", epoch)\n",
        "  bradley_model.train()\n",
        "  for i, batch in enumerate(iter(train_iterator)):\n",
        "    src = batch.query\n",
        "    trg = batch.response\n",
        "    optimizer.zero_grad()\n",
        "    out = bradley_model(src, trg)\n",
        "    loss = criterion(out[:-1,:].view(-1, out.shape[2]), trg[1:,:].view(-1))\n",
        "    if(i%100==0):\n",
        "      print(\"{}: {}: {}\".format(src.shape[1],i, loss.item()))\n",
        "    #print('current memory allocated: {}'.format(torch.cuda.memory_allocated() / 1024 ** 2))\n",
        "    #print(src.shape)\n",
        "    #print(trg.shape)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(bradley_model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "    #torch.cuda.empty_cache()\n",
        "    #del loss\n",
        "    #del src\n",
        "    #del trg\n",
        "    #del out\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6915.0625\n",
            "epoch  1\n",
            "128: 0: 10.740097045898438\n",
            "128: 100: 6.313337802886963\n",
            "128: 200: 6.489552974700928\n",
            "128: 300: 7.44293212890625\n",
            "128: 400: 6.569747447967529\n",
            "64: 500: 6.518950939178467\n",
            "128: 600: 6.071295738220215\n",
            "128: 700: 5.481566429138184\n",
            "64: 800: 6.375940322875977\n",
            "64: 900: 6.660166263580322\n",
            "64: 1000: 6.00575590133667\n",
            "64: 1100: 6.207804203033447\n",
            "32: 1200: 6.329902648925781\n",
            "32: 1300: 6.518479347229004\n",
            "32: 1400: 6.319914817810059\n",
            "32: 1500: 6.229698657989502\n",
            "32: 1600: 6.216860771179199\n",
            "128: 1700: 6.047629356384277\n",
            "64: 1800: 5.151285171508789\n",
            "64: 1900: 5.830179691314697\n",
            "32: 2000: 5.744325160980225\n",
            "32: 2100: 5.459617614746094\n",
            "32: 2200: 5.996384143829346\n",
            "32: 2300: 5.840493202209473\n",
            "32: 2400: 6.186654090881348\n",
            "32: 2500: 6.222421646118164\n",
            "32: 2600: 5.972757816314697\n",
            "32: 2700: 6.337585926055908\n",
            "32: 2800: 6.269260406494141\n",
            "32: 2900: 6.356545448303223\n",
            "16: 3000: 6.338551998138428\n",
            "16: 3100: 6.485777854919434\n",
            "16: 3200: 6.274703025817871\n",
            "32: 3300: 5.969729423522949\n",
            "32: 3400: 5.686072826385498\n",
            "32: 3500: 5.925056457519531\n",
            "32: 3600: 5.927244663238525\n",
            "16: 3700: 5.768136024475098\n",
            "16: 3800: 6.3009538650512695\n",
            "16: 3900: 6.176474571228027\n",
            "8: 4000: 6.650984287261963\n",
            "16: 4100: 5.658264636993408\n",
            "9: 4200: 6.268261909484863\n",
            "8: 4300: 5.418497085571289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58778EVIm9BS",
        "colab_type": "code",
        "outputId": "76dde8ca-a13d-43d3-e720-f5cb38b0eba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "source_sentence = train_dataset[50].query\n",
        "#source_sentence = ['what','is','?']\n",
        "print(source_sentence)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['is', 'he', 'oily', 'or', 'dry', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnxVh2pinJHH",
        "colab_type": "code",
        "outputId": "4d234210-ab99-4825-a9a3-fca7adcaae1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = QUERY.numericalize([source_sentence]).to(device)\n",
        "x = x.flatten()\n",
        "print(x.shape)\n",
        "result = bradley_model.greedy_inference_one_sample(x)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrrmT-mi-ce3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "28409208-e5d0-4aed-eb2b-f5bdd6c3ed73"
      },
      "source": [
        "result"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5],\n",
              "        [5]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EViqMhvoYA8W",
        "colab_type": "code",
        "outputId": "0f0761ff-928c-4484-dd80-14cb1bc54610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "print(RESPONSE.vocab.itos[2])\n",
        "print(RESPONSE.vocab.itos[5])\n",
        "print(RESPONSE.vocab.itos[8])\n",
        "print(RESPONSE.vocab.itos[3])\n",
        "print(RESPONSE.vocab.itos[22])\n",
        "print(RESPONSE.vocab.itos[4])\n",
        "print(RESPONSE.vocab.itos[66])\n",
        "print(RESPONSE.vocab.itos[125])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bos>\n",
            ",\n",
            "?\n",
            "<eos>\n",
            "!\n",
            ".\n",
            "think\n",
            "little\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nclzWr-trMJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f9096903-86d3-4f42-aa3c-04280df41261"
      },
      "source": [
        "def batch_index_to_strings(trg):\n",
        "  # trg = [sent_len, batch_size]\n",
        "  temp = trg.transpose(0,1)\n",
        "  for i, row in enumerate(temp):\n",
        "    print(row)\n",
        "\n",
        "batch_index_to_strings(result)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5le-q6_IrZpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "bab19e45-c89b-4c1c-bfe4-ced4a1c51c3a"
      },
      "source": [
        "mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0f73ca2d8abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sz' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13y2paU7rbA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = (torch.triu(torch.ones(5, 5)) == 1).transpose(0, 1)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10kk8xzZsBvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}