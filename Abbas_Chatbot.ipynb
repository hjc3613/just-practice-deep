{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bradley_Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/just-practice-deep/blob/master/Abbas_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45uNP7CfX7Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data import Field\n",
        "import spacy\n",
        "\n",
        "from spacy.symbols import ORTH\n",
        "my_tok = spacy.load('en')\n",
        "\n",
        "def spacy_tok(x):\n",
        "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
        "\n",
        "QUERY = Field(lower=True, tokenize=spacy_tok, init_token='<bos>', eos_token='<eos>')\n",
        "RESPONSE = Field(lower=True, tokenize=spacy_tok, is_target=True, init_token='<bos>', eos_token='<eos>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gmmeZHvZl73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available:\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8HlpeMbZnz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"}, {ORTH: \"n't\"}])\n",
        "my_tok.tokenizer.add_special_case(\"can't\", [{ORTH: \"can\"}, {ORTH: \"not\"}])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dy7TSTGZpoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset\n",
        "\n",
        "train_dataset = TabularDataset(path=\"./formatted_movie_lines.txt\", format=\"CSV\",\n",
        "                               fields=[(\"query\", QUERY),(\"response\", RESPONSE)],\n",
        "                               csv_reader_params={\"delimiter\":'\\t'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv-wQRDlNvgM",
        "colab_type": "code",
        "outputId": "87338eb6-49e2-4926-c9b8-6c941389edcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "QUERY.build_vocab(train_dataset)\n",
        "RESPONSE.build_vocab(train_dataset)\n",
        "print(\"id of 'film' in query vocab is {}\".format(QUERY.vocab.stoi['film']))\n",
        "print(\"word of id=33 in query vocab is '{}'\".format(QUERY.vocab.itos[33]))\n",
        "print(\"len of Query vocab is {}\".format(len(QUERY.vocab)))\n",
        "print(\"len of Response vocab is {}\".format(len(RESPONSE.vocab)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id of 'film' in query vocab is 1059\n",
            "word of id=33 in query vocab is 'know'\n",
            "len of Query vocab is 48505\n",
            "len of Response vocab is 49036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--vxBRleA8PV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ef433bf7-9285-4b26-a1d0-75dd98dfef56"
      },
      "source": [
        "print(\"number of rows in train data is {}\".format(len(train_dataset)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of rows in train data is 221282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfPkNvi_B1gd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2be6f62-7e80-4a36-b317-05d6c48785fb"
      },
      "source": [
        "from torchtext.data import Dataset\n",
        "\n",
        "def my_filter_pred(example, limited_word = 5):\n",
        "  if(len(example.query) < limited_word and len(example.response) < limited_word):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "phase_train_dataset = Dataset(examples = train_dataset.examples,\n",
        "               fields=[(\"query\", QUERY),(\"response\", RESPONSE)],\n",
        "               filter_pred = my_filter_pred)\n",
        "\n",
        "print(\"len of this phase_train_dataset is {}\".format(len(phase_train_dataset)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of this phase_train_dataset is 9609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEjDykrEabVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import BucketIterator, interleave_keys\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "train_iterator = BucketIterator(dataset= phase_train_dataset, batch_size=batch_size,\n",
        "                                device=device,\n",
        "                                sort_key=lambda x: interleave_keys(len(x.query), len(x.response)),\n",
        "                                sort = True,\n",
        "                                shuffle = True,\n",
        "                                repeat = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQvdKHxvCZjW",
        "colab_type": "code",
        "outputId": "572f45d3-1d58-47ac-e944-34d933dfbd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "## test if data loads well?\n",
        "for a in iter(train_iterator):\n",
        "  print(\"response shape : \\t\",a.response.shape)\n",
        "  print(\"query shape :    \\t\",a.query.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "response shape : \t torch.Size([5, 512])\n",
            "query shape :    \t torch.Size([3, 512])\n",
            "response shape : \t torch.Size([4, 512])\n",
            "query shape :    \t torch.Size([2, 512])\n",
            "response shape : \t torch.Size([4, 512])\n",
            "query shape :    \t torch.Size([2, 512])\n",
            "response shape : \t torch.Size([5, 512])\n",
            "query shape :    \t torch.Size([2, 512])\n",
            "response shape : \t torch.Size([5, 512])\n",
            "query shape :    \t torch.Size([3, 512])\n",
            "response shape : \t torch.Size([4, 512])\n",
            "query shape :    \t torch.Size([3, 512])\n",
            "response shape : \t torch.Size([5, 512])\n",
            "query shape :    \t torch.Size([3, 512])\n",
            "response shape : \t torch.Size([5, 512])\n",
            "query shape :    \t torch.Size([3, 512])\n",
            "response shape : \t torch.Size([6, 512])\n",
            "query shape :    \t torch.Size([3, 512])\n",
            "response shape : \t torch.Size([6, 512])\n",
            "query shape :    \t torch.Size([2, 512])\n",
            "response shape : \t torch.Size([6, 512])\n",
            "query shape :    \t torch.Size([3, 512])\n",
            "response shape : \t torch.Size([6, 512])\n",
            "query shape :    \t torch.Size([4, 512])\n",
            "response shape : \t torch.Size([4, 512])\n",
            "query shape :    \t torch.Size([4, 512])\n",
            "response shape : \t torch.Size([4, 512])\n",
            "query shape :    \t torch.Size([4, 512])\n",
            "response shape : \t torch.Size([5, 512])\n",
            "query shape :    \t torch.Size([4, 512])\n",
            "response shape : \t torch.Size([5, 512])\n",
            "query shape :    \t torch.Size([4, 512])\n",
            "response shape : \t torch.Size([6, 512])\n",
            "query shape :    \t torch.Size([4, 512])\n",
            "response shape : \t torch.Size([6, 512])\n",
            "query shape :    \t torch.Size([4, 512])\n",
            "response shape : \t torch.Size([6, 393])\n",
            "query shape :    \t torch.Size([4, 393])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4iHd1PvadmJ",
        "colab_type": "code",
        "outputId": "2c85e0e2-3ec2-4973-84ea-a2d15d6340ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_batch = 0\n",
        "for batch in (iter(train_iterator)):\n",
        "  num_batch += 1\n",
        "print(\"number of batch is:\", num_batch)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of batch is: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x8xba4xayJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class Abbas(nn.Module):\n",
        "  def __init__(self, src_voc_sze, trg_voc_sze, hid_sze, num_head, num_enc, num_dec):\n",
        "    super(Abbas, self).__init__()\n",
        "    self.hid_sze = hid_sze\n",
        "    self.src_word_embedding = nn.Embedding(src_voc_sze, self.hid_sze)\n",
        "    self.trg_word_embedding = nn.Embedding(trg_voc_sze, self.hid_sze)\n",
        "    self.trg_pos_embedding = nn.Embedding(800, self.hid_sze)\n",
        "    self.num_head = num_head\n",
        "    self.transformer = nn.Transformer(self.hid_sze, self.num_head, num_enc, num_dec)\n",
        "    self.fc = nn.Linear(self.hid_sze, trg_voc_sze)\n",
        "  \n",
        "  def forward(self, src, trg):\n",
        "    #src = [src sent len, batch_size]\n",
        "    #trg = [trg sent len, batch_size]\n",
        "    temp_src = self.src_word_embedding(src)\n",
        "    temp_trg = self.trg_word_embedding(trg)\n",
        "    trg_sent_len, batch_size = trg.shape[0], trg.shape[1]\n",
        "    trg_pos = self.trg_pos_embedding(torch.arange(0, trg_sent_len).unsqueeze(0).\n",
        "                                     repeat(batch_size,1).to(device)).transpose(0,1)\n",
        "    trg_mask = self._generate_square_subsequent_mask(trg_sent_len)\n",
        "    temp_trg = temp_trg + trg_pos\n",
        "    temp = self.transformer(temp_src, temp_trg, tgt_mask=trg_mask)\n",
        "    return self.fc(temp)\n",
        "  \n",
        "  def _generate_square_subsequent_mask(self, sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    mask = mask.to(device)\n",
        "    return mask\n",
        "\n",
        "\n",
        "  def greedy_inference_one_sample(self, src, max_len=20):\n",
        "\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      #src = [src sent len]\n",
        "      src_len = src.shape[0]\n",
        "      src = src.unsqueeze(1)\n",
        "      #src = [sent_len, 1]\n",
        "      trg = src.new_full((1,1), RESPONSE.vocab.stoi['<bos>'])\n",
        "      #trg = [1,1]\n",
        "\n",
        "      translation_step = 0\n",
        "      while translation_step < max_len:\n",
        "        out = self.forward(src, trg)\n",
        "        out = out[-1,:]\n",
        "        #out = [batch_size, trg_vocab_size]\n",
        "        nex = out.argmax(dim=1).unsqueeze(0)\n",
        "        #nex = [1, 1]\n",
        "        trg = torch.cat((trg, nex), dim=0)\n",
        "        translation_step += 1\n",
        "    return trg\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QgtTbBmcsNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hid_dim = 512\n",
        "src_voc_size = len(QUERY.vocab)\n",
        "trg_voc_size = len(RESPONSE.vocab)\n",
        "num_head = 8\n",
        "num_enc = 6\n",
        "num_dec = 4\n",
        "\n",
        "abbas_model = Abbas(src_voc_sze=src_voc_size, trg_voc_sze=trg_voc_size,\n",
        "                        hid_sze=hid_dim, num_head=num_head,\n",
        "                        num_enc=num_enc, num_dec=num_dec)\n",
        "abbas_model = abbas_model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrHmyiE6HMFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd1c2946-2f6f-48e9-c2ef-5e370d8cd5f5"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(abbas_model):,} trainable parameters')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 111,238,540 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S9HHR9lHjc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in abbas_model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqrmC-BkHlG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CFjFowyvF88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Define criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=RESPONSE.vocab.stoi['<pad>'])\n",
        "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
        "            torch.optim.Adam(abbas_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K00UxkjTvL92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_one_epoch():\n",
        "  epoch_loss = 0\n",
        "  CLIP = 1\n",
        "  abbas_model.train()\n",
        "  for i, batch in enumerate(iter(train_iterator)):\n",
        "    src = batch.query\n",
        "    trg = batch.response\n",
        "    optimizer.zero_grad()\n",
        "    out = abbas_model(src, trg)\n",
        "    loss = criterion(out[:-1,:].view(-1, out.shape[2]), trg[1:,:].view(-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(abbas_model.parameters(), CLIP)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss / len(train_iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNtwt_LuvrAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_number\n",
        "for epoch in range(1, epoch_number+1):\n",
        "  epoch_loss = train_one_epoch()\n",
        "  print(\"epoch {} loss is: {}\".format(epoch, epoch_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr8hvOmTeGUy",
        "colab_type": "code",
        "outputId": "0fd55016-7ec1-461e-9c68-12d009a61c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=RESPONSE.vocab.stoi['<pad>'])\n",
        "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
        "            torch.optim.Adam(abbas_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "epoch_number = 100\n",
        "\n",
        "for epoch in range(1, epoch_number+1):\n",
        "  epoch_loss = 0\n",
        "  abbas_model.train()\n",
        "  for i, batch in enumerate(iter(train_iterator)):\n",
        "    src = batch.query\n",
        "    trg = batch.response\n",
        "    optimizer.zero_grad()\n",
        "    out = abbas_model(src, trg)\n",
        "    loss = criterion(out[:-1,:].view(-1, out.shape[2]), trg[1:,:].view(-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(abbas_model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  avr_epoch_loss = epoch_loss / len(train_iterator)\n",
        "  print(\"epoch {} loss is: {}\".format(epoch, avr_epoch_loss))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 loss is: 5.603388943170247\n",
            "epoch 2 loss is: 4.753289473684211\n",
            "epoch 3 loss is: 3.9632382675221094\n",
            "epoch 4 loss is: 3.5428546855324194\n",
            "epoch 5 loss is: 3.2241162626366866\n",
            "epoch 6 loss is: 2.896531873627713\n",
            "epoch 7 loss is: 2.649756135124909\n",
            "epoch 8 loss is: 2.37518142399035\n",
            "epoch 9 loss is: 2.1033928096294403\n",
            "epoch 10 loss is: 1.9118904392970235\n",
            "epoch 11 loss is: 1.7879091579663127\n",
            "epoch 12 loss is: 1.704604839023791\n",
            "epoch 13 loss is: 1.5963323131987923\n",
            "epoch 14 loss is: 1.503433669083997\n",
            "epoch 15 loss is: 1.4165270375578027\n",
            "epoch 16 loss is: 1.3538921710691953\n",
            "epoch 17 loss is: 1.300322803227525\n",
            "epoch 18 loss is: 1.2105453477094048\n",
            "epoch 19 loss is: 1.1232946060205762\n",
            "epoch 20 loss is: 1.0136110747331066\n",
            "epoch 21 loss is: 0.9533794283082611\n",
            "epoch 22 loss is: 0.8727059195700445\n",
            "epoch 23 loss is: 0.8038695862418727\n",
            "epoch 24 loss is: 0.7317110425547549\n",
            "epoch 25 loss is: 0.6618704966416484\n",
            "epoch 26 loss is: 0.5975270743824934\n",
            "epoch 27 loss is: 0.5550994657372174\n",
            "epoch 28 loss is: 0.5122555960950098\n",
            "epoch 29 loss is: 0.45867889501938697\n",
            "epoch 30 loss is: 0.41464821161016036\n",
            "epoch 31 loss is: 0.3779289180314855\n",
            "epoch 32 loss is: 0.33549334667623043\n",
            "epoch 33 loss is: 0.31107692067560394\n",
            "epoch 34 loss is: 0.26827264381082433\n",
            "epoch 35 loss is: 0.23694517071309842\n",
            "epoch 36 loss is: 0.20468057329325298\n",
            "epoch 37 loss is: 0.17561690764207588\n",
            "epoch 38 loss is: 0.1499941957330233\n",
            "epoch 39 loss is: 0.12181363387131378\n",
            "epoch 40 loss is: 0.10052962709022195\n",
            "epoch 41 loss is: 0.07515738223140177\n",
            "epoch 42 loss is: 0.06142185086776551\n",
            "epoch 43 loss is: 0.047301502457182655\n",
            "epoch 44 loss is: 0.036637988113062944\n",
            "epoch 45 loss is: 0.028160442855503214\n",
            "epoch 46 loss is: 0.01893425024555702\n",
            "epoch 47 loss is: 0.012413840189478114\n",
            "epoch 48 loss is: 0.008084909812743334\n",
            "epoch 49 loss is: 0.0053548040971356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-cd78d0915331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabbas_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabbas_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58778EVIm9BS",
        "colab_type": "code",
        "outputId": "e25ce96a-8c5f-4386-f0c3-20176afa18ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#source_sentence = phase_train_dataset[40].query\n",
        "source_sentence = ['hello']\n",
        "print(source_sentence)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnxVh2pinJHH",
        "colab_type": "code",
        "outputId": "10dc8219-f082-4ddb-a9c5-41cda2720c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "x = QUERY.numericalize([source_sentence]).to(device)\n",
        "x = x.flatten()\n",
        "print(x.shape)\n",
        "result = abbas_model.greedy_inference_one_sample(x)\n",
        "result = result.flatten()\n",
        "for wrd_ind in result:\n",
        "  print(RESPONSE.vocab.itos[wrd_ind])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1])\n",
            "<bos>\n",
            "ahh\n",
            "ahh\n",
            "ahh\n",
            "ahh\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n",
            "<eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nclzWr-trMJe",
        "colab_type": "code",
        "outputId": "f9096903-86d3-4f42-aa3c-04280df41261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def batch_index_to_strings(trg):\n",
        "  # trg = [sent_len, batch_size]\n",
        "  temp = trg.transpose(0,1)\n",
        "  for i, row in enumerate(temp):\n",
        "    print(row)\n",
        "\n",
        "batch_index_to_strings(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5le-q6_IrZpa",
        "colab_type": "code",
        "outputId": "bab19e45-c89b-4c1c-bfe4-ced4a1c51c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0f73ca2d8abc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sz' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13y2paU7rbA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = (torch.triu(torch.ones(5, 5)) == 1).transpose(0, 1)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10kk8xzZsBvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}