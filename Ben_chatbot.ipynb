{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ben_chatbot.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/just-practice-deep/blob/master/Ben_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuwcHEtHLoJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm51ZP8QLsmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za5C9cGRLvIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7uWTZcoMeXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
        "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzWZr90jOrWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset, interleave_keys\n",
        "\n",
        "train_dataset = TabularDataset(path=\"./formatted_movie_lines.txt\", format=\"CSV\",\n",
        "                               fields=[(\"query\", SRC),(\"response\", TRG)],\n",
        "                               csv_reader_params={\"delimiter\":'\\t'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIsQ-_c9zHJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_dataset, min_freq=2)\n",
        "TRG.build_vocab(train_dataset, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4gwC5BWTlHU",
        "colab_type": "code",
        "outputId": "b68a5cea-b9d0-45b7-cb3f-9b46bdf322ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221282"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SydmR2II11_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1cO96vzQLU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_backup_examples = train_dataset.examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHja9Fi2V5SE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_limited_dataset(num_word):\n",
        "  temp = []\n",
        "  print(len(temp))\n",
        "  for i, example in enumerate(train_backup_examples):\n",
        "    if( (len(example.query) < num_word) and (len(example.response) < num_word) ):\n",
        "      temp.append(example)\n",
        "  print(len(temp))\n",
        "  train_dataset.examples = temp   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnfnyaqbQgAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "27eac8d6-d664-4e82-da2f-0128073d8165"
      },
      "source": [
        "_get_limited_dataset(4)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "4212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWo7y37kUXSY",
        "colab_type": "code",
        "outputId": "4823db90-ad11-4d47-c426-6ec2e83e5ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4212"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-GMwUtrQbMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "15082549-a05a-40b0-9f63-8f4f91352c4e"
      },
      "source": [
        "len(train_backup_examples)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221282"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuRQi9ugMj5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iKzyx1HMlij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "\n",
        "train_iterator = BucketIterator(train_dataset,\n",
        "     batch_size=BATCH_SIZE,\n",
        "     sort_key=lambda x: interleave_keys(len(x.query), len(x.response)),\n",
        "     sort = True,\n",
        "     device=device,\n",
        "     shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsikYpBEZ-jq",
        "colab_type": "code",
        "outputId": "462a9973-a5f5-43d0-a46b-8382006d9eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(list(iter(train_iterator)))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxBLv7pBaLio",
        "colab_type": "code",
        "outputId": "920bd6b7-9b39-486e-e243-10f71a36928c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "for batch in iter(train_iterator):\n",
        "  print(batch.query.shape)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1024, 5])\n",
            "torch.Size([1024, 4])\n",
            "torch.Size([1024, 5])\n",
            "torch.Size([1024, 5])\n",
            "torch.Size([116, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrSqNwgkMoxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, encoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.encoder_layer = encoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([encoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
        "        \n",
        "        src = self.do((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuY02fB8Mqlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        src = self.ln(src + self.do(self.sa(src, src, src, src_mask)))\n",
        "        \n",
        "        src = self.ln(src + self.do(self.pf(src)))\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYdW9KvgMuaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \n",
        "        bsz = query.shape[0]\n",
        "        \n",
        "        #query = key = value [batch size, sent len, hid dim]\n",
        "                \n",
        "        Q = self.w_q(query)\n",
        "        K = self.w_k(key)\n",
        "        V = self.w_v(value)\n",
        "        \n",
        "        #Q, K, V = [batch size, sent len, hid dim]\n",
        "        \n",
        "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q, K, V = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = self.do(torch.softmax(energy, dim=-1))\n",
        "        \n",
        "        #attention = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        x = torch.matmul(attention, V)\n",
        "        \n",
        "        #x = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, sent len, n heads, hid dim // n heads]\n",
        "        \n",
        "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
        "        \n",
        "        #x = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zZ34q5MMwip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.pf_dim = pf_dim\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        x = self.do(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, sent len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMeo6hh4My4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, decoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.decoder_layer = decoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([decoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch_size, trg sent len]\n",
        "        #src = [batch_size, src sent len]\n",
        "        #trg_mask = [batch size, trg sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(self.device)\n",
        "                \n",
        "        trg = self.do((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #trg = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "            \n",
        "        return self.fc(trg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8sg-1xQM034",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.ea = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg sent len, hid dim]\n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        #trg_mask = [batch size, trg sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "                \n",
        "        trg = self.ln(trg + self.do(self.sa(trg, trg, trg, trg_mask)))\n",
        "                \n",
        "        trg = self.ln(trg + self.do(self.ea(trg, src, src, src_mask)))\n",
        "        \n",
        "        trg = self.ln(trg + self.do(self.pf(trg)))\n",
        "        \n",
        "        return trg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4cAKNzM3gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, sos_idx, pad_idx, device, maxlen=50):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sos_idx = sos_idx\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "    def make_masks(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "        \n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src sent len]\n",
        "        #trg_pad_mask = [batch size, 1, trg sent len, 1]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
        "                        \n",
        "        #trg_sub_mask = [trg sent len, trg sent len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg sent len, trg sent len]\n",
        "        \n",
        "        return src_mask, trg_mask\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "                \n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src sent len, hid dim]\n",
        "                \n",
        "        out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #out = [batch size, trg sent len, output dim]\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def translate_sequences(self, src):\n",
        "        #src = [batch size, src sent len]\n",
        "        \n",
        "        batch_size, src_len = src.shape\n",
        "        trg = src.new_full((batch_size, 1), self.sos_idx)\n",
        "        #trg = [batch size, 1]\n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        translation_step = 0\n",
        "        while translation_step < self.maxlen:\n",
        "            out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "            # out - [batch size, trg sent len, output dim]\n",
        "            out = torch.argmax(out[:, -1], dim=1) # batch size\n",
        "            out = out.unsqueeze(1) # batch size, 1\n",
        "            trg = torch.cat((trg, out), dim=1)\n",
        "            # trg - [batch size, trg sent len]\n",
        "            src_mask, trg_mask = self.make_masks(src, trg)\n",
        "            translation_step += 1\n",
        "        return trg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSA5dd91M4X4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = len(SRC.vocab)\n",
        "hid_dim = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "enc = Encoder(input_dim, hid_dim, n_layers, n_heads, pf_dim, EncoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q0ndCVuM71R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dim = len(TRG.vocab)\n",
        "hid_dim = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "dec = Decoder(output_dim, hid_dim, n_layers, n_heads, pf_dim, DecoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0E40UH2M9tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = SRC.vocab.stoi['<pad>']\n",
        "sos_idx = SRC.vocab.stoi['<sos>']\n",
        "\n",
        "model = Seq2Seq(enc, dec, sos_idx, pad_idx, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw_x-s0QM_X_",
        "colab_type": "code",
        "outputId": "a8457154-b4c9-4566-c489-28373438a227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 87,693,463 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDy7Zri9NBBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVCU3TO0NDAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8lurD8ANE2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOET3uYNGob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mntX38RPNI0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.query\n",
        "        trg = batch.response\n",
        "        \n",
        "        #if(i%100==1):\n",
        "        #  print(\"src shape {}->batch {}: {}\".format(src.shape,i, loss))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg sent len - 1, output dim]\n",
        "        #trg = [batch size, trg sent len]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg sent len - 1, output dim]\n",
        "        #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACddO2tRNK-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg sent len - 1, output dim]\n",
        "            #trg = [batch size, trg sent len]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg sent len - 1, output dim]\n",
        "            #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0tWkAHGNNSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THqEDT0fNPVa",
        "colab_type": "code",
        "outputId": "7670a00a-88f4-46a6-a453-1d12b53f3e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N_EPOCHS = 500\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 5s\n",
            "\tTrain Loss: 5.474 | Train PPL: 238.400\n",
            "Epoch: 02 | Time: 0m 5s\n",
            "\tTrain Loss: 5.417 | Train PPL: 225.267\n",
            "Epoch: 03 | Time: 0m 5s\n",
            "\tTrain Loss: 5.272 | Train PPL: 194.718\n",
            "Epoch: 04 | Time: 0m 5s\n",
            "\tTrain Loss: 5.100 | Train PPL: 164.056\n",
            "Epoch: 05 | Time: 0m 5s\n",
            "\tTrain Loss: 4.899 | Train PPL: 134.183\n",
            "Epoch: 06 | Time: 0m 5s\n",
            "\tTrain Loss: 4.646 | Train PPL: 104.180\n",
            "Epoch: 07 | Time: 0m 5s\n",
            "\tTrain Loss: 4.405 | Train PPL:  81.851\n",
            "Epoch: 08 | Time: 0m 5s\n",
            "\tTrain Loss: 4.151 | Train PPL:  63.501\n",
            "Epoch: 09 | Time: 0m 5s\n",
            "\tTrain Loss: 3.908 | Train PPL:  49.787\n",
            "Epoch: 10 | Time: 0m 5s\n",
            "\tTrain Loss: 3.745 | Train PPL:  42.325\n",
            "Epoch: 11 | Time: 0m 5s\n",
            "\tTrain Loss: 3.610 | Train PPL:  36.974\n",
            "Epoch: 12 | Time: 0m 5s\n",
            "\tTrain Loss: 3.493 | Train PPL:  32.871\n",
            "Epoch: 13 | Time: 0m 5s\n",
            "\tTrain Loss: 3.402 | Train PPL:  30.011\n",
            "Epoch: 14 | Time: 0m 5s\n",
            "\tTrain Loss: 3.318 | Train PPL:  27.597\n",
            "Epoch: 15 | Time: 0m 5s\n",
            "\tTrain Loss: 3.233 | Train PPL:  25.348\n",
            "Epoch: 16 | Time: 0m 5s\n",
            "\tTrain Loss: 3.155 | Train PPL:  23.452\n",
            "Epoch: 17 | Time: 0m 5s\n",
            "\tTrain Loss: 3.079 | Train PPL:  21.741\n",
            "Epoch: 18 | Time: 0m 5s\n",
            "\tTrain Loss: 3.011 | Train PPL:  20.316\n",
            "Epoch: 19 | Time: 0m 5s\n",
            "\tTrain Loss: 2.938 | Train PPL:  18.885\n",
            "Epoch: 20 | Time: 0m 5s\n",
            "\tTrain Loss: 2.874 | Train PPL:  17.708\n",
            "Epoch: 21 | Time: 0m 5s\n",
            "\tTrain Loss: 2.796 | Train PPL:  16.384\n",
            "Epoch: 22 | Time: 0m 5s\n",
            "\tTrain Loss: 2.735 | Train PPL:  15.405\n",
            "Epoch: 23 | Time: 0m 5s\n",
            "\tTrain Loss: 2.664 | Train PPL:  14.359\n",
            "Epoch: 24 | Time: 0m 5s\n",
            "\tTrain Loss: 2.585 | Train PPL:  13.263\n",
            "Epoch: 25 | Time: 0m 5s\n",
            "\tTrain Loss: 2.518 | Train PPL:  12.407\n",
            "Epoch: 26 | Time: 0m 5s\n",
            "\tTrain Loss: 2.444 | Train PPL:  11.520\n",
            "Epoch: 27 | Time: 0m 5s\n",
            "\tTrain Loss: 2.378 | Train PPL:  10.784\n",
            "Epoch: 28 | Time: 0m 5s\n",
            "\tTrain Loss: 2.318 | Train PPL:  10.150\n",
            "Epoch: 29 | Time: 0m 5s\n",
            "\tTrain Loss: 2.259 | Train PPL:   9.574\n",
            "Epoch: 30 | Time: 0m 5s\n",
            "\tTrain Loss: 2.193 | Train PPL:   8.958\n",
            "Epoch: 31 | Time: 0m 5s\n",
            "\tTrain Loss: 2.137 | Train PPL:   8.478\n",
            "Epoch: 32 | Time: 0m 5s\n",
            "\tTrain Loss: 2.089 | Train PPL:   8.079\n",
            "Epoch: 33 | Time: 0m 5s\n",
            "\tTrain Loss: 2.039 | Train PPL:   7.679\n",
            "Epoch: 34 | Time: 0m 5s\n",
            "\tTrain Loss: 1.988 | Train PPL:   7.299\n",
            "Epoch: 35 | Time: 0m 5s\n",
            "\tTrain Loss: 1.941 | Train PPL:   6.965\n",
            "Epoch: 36 | Time: 0m 5s\n",
            "\tTrain Loss: 1.899 | Train PPL:   6.677\n",
            "Epoch: 37 | Time: 0m 5s\n",
            "\tTrain Loss: 1.865 | Train PPL:   6.455\n",
            "Epoch: 38 | Time: 0m 5s\n",
            "\tTrain Loss: 1.843 | Train PPL:   6.314\n",
            "Epoch: 39 | Time: 0m 5s\n",
            "\tTrain Loss: 1.790 | Train PPL:   5.991\n",
            "Epoch: 40 | Time: 0m 5s\n",
            "\tTrain Loss: 1.767 | Train PPL:   5.851\n",
            "Epoch: 41 | Time: 0m 5s\n",
            "\tTrain Loss: 1.734 | Train PPL:   5.666\n",
            "Epoch: 42 | Time: 0m 5s\n",
            "\tTrain Loss: 1.708 | Train PPL:   5.516\n",
            "Epoch: 43 | Time: 0m 5s\n",
            "\tTrain Loss: 1.687 | Train PPL:   5.406\n",
            "Epoch: 44 | Time: 0m 5s\n",
            "\tTrain Loss: 1.652 | Train PPL:   5.215\n",
            "Epoch: 45 | Time: 0m 5s\n",
            "\tTrain Loss: 1.616 | Train PPL:   5.034\n",
            "Epoch: 46 | Time: 0m 5s\n",
            "\tTrain Loss: 1.621 | Train PPL:   5.057\n",
            "Epoch: 47 | Time: 0m 5s\n",
            "\tTrain Loss: 1.581 | Train PPL:   4.861\n",
            "Epoch: 48 | Time: 0m 5s\n",
            "\tTrain Loss: 1.550 | Train PPL:   4.712\n",
            "Epoch: 49 | Time: 0m 5s\n",
            "\tTrain Loss: 1.543 | Train PPL:   4.677\n",
            "Epoch: 50 | Time: 0m 5s\n",
            "\tTrain Loss: 1.513 | Train PPL:   4.539\n",
            "Epoch: 51 | Time: 0m 5s\n",
            "\tTrain Loss: 1.506 | Train PPL:   4.510\n",
            "Epoch: 52 | Time: 0m 5s\n",
            "\tTrain Loss: 1.495 | Train PPL:   4.461\n",
            "Epoch: 53 | Time: 0m 5s\n",
            "\tTrain Loss: 1.463 | Train PPL:   4.321\n",
            "Epoch: 54 | Time: 0m 5s\n",
            "\tTrain Loss: 1.435 | Train PPL:   4.201\n",
            "Epoch: 55 | Time: 0m 5s\n",
            "\tTrain Loss: 1.420 | Train PPL:   4.139\n",
            "Epoch: 56 | Time: 0m 5s\n",
            "\tTrain Loss: 1.392 | Train PPL:   4.021\n",
            "Epoch: 57 | Time: 0m 5s\n",
            "\tTrain Loss: 1.380 | Train PPL:   3.975\n",
            "Epoch: 58 | Time: 0m 5s\n",
            "\tTrain Loss: 1.349 | Train PPL:   3.852\n",
            "Epoch: 59 | Time: 0m 5s\n",
            "\tTrain Loss: 1.321 | Train PPL:   3.746\n",
            "Epoch: 60 | Time: 0m 5s\n",
            "\tTrain Loss: 1.314 | Train PPL:   3.722\n",
            "Epoch: 61 | Time: 0m 5s\n",
            "\tTrain Loss: 1.283 | Train PPL:   3.606\n",
            "Epoch: 62 | Time: 0m 5s\n",
            "\tTrain Loss: 1.262 | Train PPL:   3.533\n",
            "Epoch: 63 | Time: 0m 5s\n",
            "\tTrain Loss: 1.257 | Train PPL:   3.514\n",
            "Epoch: 64 | Time: 0m 5s\n",
            "\tTrain Loss: 1.213 | Train PPL:   3.364\n",
            "Epoch: 65 | Time: 0m 5s\n",
            "\tTrain Loss: 1.216 | Train PPL:   3.372\n",
            "Epoch: 66 | Time: 0m 5s\n",
            "\tTrain Loss: 1.193 | Train PPL:   3.297\n",
            "Epoch: 67 | Time: 0m 5s\n",
            "\tTrain Loss: 1.187 | Train PPL:   3.278\n",
            "Epoch: 68 | Time: 0m 5s\n",
            "\tTrain Loss: 1.161 | Train PPL:   3.193\n",
            "Epoch: 69 | Time: 0m 5s\n",
            "\tTrain Loss: 1.139 | Train PPL:   3.123\n",
            "Epoch: 70 | Time: 0m 5s\n",
            "\tTrain Loss: 1.112 | Train PPL:   3.041\n",
            "Epoch: 71 | Time: 0m 5s\n",
            "\tTrain Loss: 1.085 | Train PPL:   2.960\n",
            "Epoch: 72 | Time: 0m 5s\n",
            "\tTrain Loss: 1.063 | Train PPL:   2.894\n",
            "Epoch: 73 | Time: 0m 5s\n",
            "\tTrain Loss: 1.050 | Train PPL:   2.857\n",
            "Epoch: 74 | Time: 0m 5s\n",
            "\tTrain Loss: 1.031 | Train PPL:   2.804\n",
            "Epoch: 75 | Time: 0m 5s\n",
            "\tTrain Loss: 1.004 | Train PPL:   2.729\n",
            "Epoch: 76 | Time: 0m 5s\n",
            "\tTrain Loss: 0.970 | Train PPL:   2.639\n",
            "Epoch: 77 | Time: 0m 5s\n",
            "\tTrain Loss: 0.961 | Train PPL:   2.613\n",
            "Epoch: 78 | Time: 0m 5s\n",
            "\tTrain Loss: 0.931 | Train PPL:   2.538\n",
            "Epoch: 79 | Time: 0m 5s\n",
            "\tTrain Loss: 0.912 | Train PPL:   2.488\n",
            "Epoch: 80 | Time: 0m 5s\n",
            "\tTrain Loss: 0.891 | Train PPL:   2.437\n",
            "Epoch: 81 | Time: 0m 5s\n",
            "\tTrain Loss: 0.873 | Train PPL:   2.394\n",
            "Epoch: 82 | Time: 0m 5s\n",
            "\tTrain Loss: 0.857 | Train PPL:   2.357\n",
            "Epoch: 83 | Time: 0m 5s\n",
            "\tTrain Loss: 0.839 | Train PPL:   2.313\n",
            "Epoch: 84 | Time: 0m 5s\n",
            "\tTrain Loss: 0.831 | Train PPL:   2.296\n",
            "Epoch: 85 | Time: 0m 5s\n",
            "\tTrain Loss: 0.809 | Train PPL:   2.245\n",
            "Epoch: 86 | Time: 0m 5s\n",
            "\tTrain Loss: 0.805 | Train PPL:   2.238\n",
            "Epoch: 87 | Time: 0m 5s\n",
            "\tTrain Loss: 0.786 | Train PPL:   2.194\n",
            "Epoch: 88 | Time: 0m 5s\n",
            "\tTrain Loss: 0.779 | Train PPL:   2.179\n",
            "Epoch: 89 | Time: 0m 5s\n",
            "\tTrain Loss: 0.777 | Train PPL:   2.176\n",
            "Epoch: 90 | Time: 0m 5s\n",
            "\tTrain Loss: 0.756 | Train PPL:   2.130\n",
            "Epoch: 91 | Time: 0m 5s\n",
            "\tTrain Loss: 0.760 | Train PPL:   2.138\n",
            "Epoch: 92 | Time: 0m 5s\n",
            "\tTrain Loss: 0.727 | Train PPL:   2.069\n",
            "Epoch: 93 | Time: 0m 5s\n",
            "\tTrain Loss: 0.711 | Train PPL:   2.035\n",
            "Epoch: 94 | Time: 0m 5s\n",
            "\tTrain Loss: 0.686 | Train PPL:   1.986\n",
            "Epoch: 95 | Time: 0m 5s\n",
            "\tTrain Loss: 0.670 | Train PPL:   1.953\n",
            "Epoch: 96 | Time: 0m 5s\n",
            "\tTrain Loss: 0.660 | Train PPL:   1.936\n",
            "Epoch: 97 | Time: 0m 5s\n",
            "\tTrain Loss: 0.651 | Train PPL:   1.918\n",
            "Epoch: 98 | Time: 0m 5s\n",
            "\tTrain Loss: 0.641 | Train PPL:   1.899\n",
            "Epoch: 99 | Time: 0m 5s\n",
            "\tTrain Loss: 0.628 | Train PPL:   1.873\n",
            "Epoch: 100 | Time: 0m 5s\n",
            "\tTrain Loss: 0.621 | Train PPL:   1.861\n",
            "Epoch: 101 | Time: 0m 5s\n",
            "\tTrain Loss: 0.618 | Train PPL:   1.855\n",
            "Epoch: 102 | Time: 0m 5s\n",
            "\tTrain Loss: 0.602 | Train PPL:   1.825\n",
            "Epoch: 103 | Time: 0m 5s\n",
            "\tTrain Loss: 0.600 | Train PPL:   1.822\n",
            "Epoch: 104 | Time: 0m 5s\n",
            "\tTrain Loss: 0.578 | Train PPL:   1.782\n",
            "Epoch: 105 | Time: 0m 5s\n",
            "\tTrain Loss: 0.571 | Train PPL:   1.769\n",
            "Epoch: 106 | Time: 0m 5s\n",
            "\tTrain Loss: 0.566 | Train PPL:   1.762\n",
            "Epoch: 107 | Time: 0m 5s\n",
            "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
            "Epoch: 108 | Time: 0m 5s\n",
            "\tTrain Loss: 0.580 | Train PPL:   1.786\n",
            "Epoch: 109 | Time: 0m 5s\n",
            "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
            "Epoch: 110 | Time: 0m 5s\n",
            "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
            "Epoch: 111 | Time: 0m 5s\n",
            "\tTrain Loss: 0.549 | Train PPL:   1.732\n",
            "Epoch: 112 | Time: 0m 5s\n",
            "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
            "Epoch: 113 | Time: 0m 5s\n",
            "\tTrain Loss: 0.524 | Train PPL:   1.689\n",
            "Epoch: 114 | Time: 0m 5s\n",
            "\tTrain Loss: 0.520 | Train PPL:   1.683\n",
            "Epoch: 115 | Time: 0m 5s\n",
            "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
            "Epoch: 116 | Time: 0m 5s\n",
            "\tTrain Loss: 0.500 | Train PPL:   1.648\n",
            "Epoch: 117 | Time: 0m 5s\n",
            "\tTrain Loss: 0.492 | Train PPL:   1.635\n",
            "Epoch: 118 | Time: 0m 5s\n",
            "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
            "Epoch: 119 | Time: 0m 5s\n",
            "\tTrain Loss: 0.496 | Train PPL:   1.642\n",
            "Epoch: 120 | Time: 0m 5s\n",
            "\tTrain Loss: 0.496 | Train PPL:   1.641\n",
            "Epoch: 121 | Time: 0m 5s\n",
            "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
            "Epoch: 122 | Time: 0m 5s\n",
            "\tTrain Loss: 0.479 | Train PPL:   1.615\n",
            "Epoch: 123 | Time: 0m 5s\n",
            "\tTrain Loss: 0.514 | Train PPL:   1.671\n",
            "Epoch: 124 | Time: 0m 5s\n",
            "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
            "Epoch: 125 | Time: 0m 5s\n",
            "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
            "Epoch: 126 | Time: 0m 5s\n",
            "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
            "Epoch: 127 | Time: 0m 5s\n",
            "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
            "Epoch: 128 | Time: 0m 5s\n",
            "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
            "Epoch: 129 | Time: 0m 5s\n",
            "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
            "Epoch: 130 | Time: 0m 5s\n",
            "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
            "Epoch: 131 | Time: 0m 5s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
            "Epoch: 132 | Time: 0m 5s\n",
            "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
            "Epoch: 133 | Time: 0m 5s\n",
            "\tTrain Loss: 0.449 | Train PPL:   1.567\n",
            "Epoch: 134 | Time: 0m 5s\n",
            "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
            "Epoch: 135 | Time: 0m 5s\n",
            "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
            "Epoch: 136 | Time: 0m 5s\n",
            "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
            "Epoch: 137 | Time: 0m 5s\n",
            "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
            "Epoch: 138 | Time: 0m 5s\n",
            "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
            "Epoch: 139 | Time: 0m 5s\n",
            "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
            "Epoch: 140 | Time: 0m 5s\n",
            "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
            "Epoch: 141 | Time: 0m 5s\n",
            "\tTrain Loss: 0.430 | Train PPL:   1.538\n",
            "Epoch: 142 | Time: 0m 5s\n",
            "\tTrain Loss: 0.425 | Train PPL:   1.529\n",
            "Epoch: 143 | Time: 0m 5s\n",
            "\tTrain Loss: 0.423 | Train PPL:   1.526\n",
            "Epoch: 144 | Time: 0m 5s\n",
            "\tTrain Loss: 0.437 | Train PPL:   1.549\n",
            "Epoch: 145 | Time: 0m 5s\n",
            "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
            "Epoch: 146 | Time: 0m 5s\n",
            "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
            "Epoch: 147 | Time: 0m 5s\n",
            "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
            "Epoch: 148 | Time: 0m 5s\n",
            "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
            "Epoch: 149 | Time: 0m 5s\n",
            "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
            "Epoch: 150 | Time: 0m 5s\n",
            "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
            "Epoch: 151 | Time: 0m 5s\n",
            "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
            "Epoch: 152 | Time: 0m 5s\n",
            "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
            "Epoch: 153 | Time: 0m 5s\n",
            "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
            "Epoch: 154 | Time: 0m 5s\n",
            "\tTrain Loss: 0.422 | Train PPL:   1.524\n",
            "Epoch: 155 | Time: 0m 5s\n",
            "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
            "Epoch: 156 | Time: 0m 5s\n",
            "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
            "Epoch: 157 | Time: 0m 5s\n",
            "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
            "Epoch: 158 | Time: 0m 5s\n",
            "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
            "Epoch: 159 | Time: 0m 5s\n",
            "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
            "Epoch: 160 | Time: 0m 5s\n",
            "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
            "Epoch: 161 | Time: 0m 5s\n",
            "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
            "Epoch: 162 | Time: 0m 5s\n",
            "\tTrain Loss: 0.408 | Train PPL:   1.503\n",
            "Epoch: 163 | Time: 0m 5s\n",
            "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
            "Epoch: 164 | Time: 0m 5s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
            "Epoch: 165 | Time: 0m 5s\n",
            "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
            "Epoch: 166 | Time: 0m 5s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
            "Epoch: 167 | Time: 0m 5s\n",
            "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
            "Epoch: 168 | Time: 0m 5s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
            "Epoch: 169 | Time: 0m 5s\n",
            "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
            "Epoch: 170 | Time: 0m 5s\n",
            "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
            "Epoch: 171 | Time: 0m 5s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
            "Epoch: 172 | Time: 0m 5s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
            "Epoch: 173 | Time: 0m 5s\n",
            "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
            "Epoch: 174 | Time: 0m 5s\n",
            "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
            "Epoch: 175 | Time: 0m 5s\n",
            "\tTrain Loss: 0.388 | Train PPL:   1.475\n",
            "Epoch: 176 | Time: 0m 5s\n",
            "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
            "Epoch: 177 | Time: 0m 5s\n",
            "\tTrain Loss: 0.399 | Train PPL:   1.491\n",
            "Epoch: 178 | Time: 0m 5s\n",
            "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
            "Epoch: 179 | Time: 0m 5s\n",
            "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
            "Epoch: 180 | Time: 0m 5s\n",
            "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
            "Epoch: 181 | Time: 0m 5s\n",
            "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
            "Epoch: 182 | Time: 0m 5s\n",
            "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
            "Epoch: 183 | Time: 0m 5s\n",
            "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
            "Epoch: 184 | Time: 0m 5s\n",
            "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
            "Epoch: 185 | Time: 0m 5s\n",
            "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
            "Epoch: 186 | Time: 0m 5s\n",
            "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
            "Epoch: 187 | Time: 0m 5s\n",
            "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
            "Epoch: 188 | Time: 0m 5s\n",
            "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
            "Epoch: 189 | Time: 0m 5s\n",
            "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
            "Epoch: 190 | Time: 0m 5s\n",
            "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
            "Epoch: 191 | Time: 0m 5s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
            "Epoch: 192 | Time: 0m 5s\n",
            "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
            "Epoch: 193 | Time: 0m 5s\n",
            "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
            "Epoch: 194 | Time: 0m 5s\n",
            "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
            "Epoch: 195 | Time: 0m 5s\n",
            "\tTrain Loss: 0.393 | Train PPL:   1.481\n",
            "Epoch: 196 | Time: 0m 5s\n",
            "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
            "Epoch: 197 | Time: 0m 5s\n",
            "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
            "Epoch: 198 | Time: 0m 5s\n",
            "\tTrain Loss: 0.408 | Train PPL:   1.503\n",
            "Epoch: 199 | Time: 0m 5s\n",
            "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
            "Epoch: 200 | Time: 0m 5s\n",
            "\tTrain Loss: 0.388 | Train PPL:   1.475\n",
            "Epoch: 201 | Time: 0m 5s\n",
            "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
            "Epoch: 202 | Time: 0m 5s\n",
            "\tTrain Loss: 0.388 | Train PPL:   1.475\n",
            "Epoch: 203 | Time: 0m 5s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.479\n",
            "Epoch: 204 | Time: 0m 5s\n",
            "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
            "Epoch: 205 | Time: 0m 5s\n",
            "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
            "Epoch: 206 | Time: 0m 5s\n",
            "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
            "Epoch: 207 | Time: 0m 5s\n",
            "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
            "Epoch: 208 | Time: 0m 5s\n",
            "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
            "Epoch: 209 | Time: 0m 5s\n",
            "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
            "Epoch: 210 | Time: 0m 5s\n",
            "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
            "Epoch: 211 | Time: 0m 5s\n",
            "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
            "Epoch: 212 | Time: 0m 5s\n",
            "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
            "Epoch: 213 | Time: 0m 5s\n",
            "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
            "Epoch: 214 | Time: 0m 5s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
            "Epoch: 215 | Time: 0m 5s\n",
            "\tTrain Loss: 0.383 | Train PPL:   1.466\n",
            "Epoch: 216 | Time: 0m 5s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
            "Epoch: 217 | Time: 0m 5s\n",
            "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
            "Epoch: 218 | Time: 0m 5s\n",
            "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
            "Epoch: 219 | Time: 0m 5s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
            "Epoch: 220 | Time: 0m 5s\n",
            "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
            "Epoch: 221 | Time: 0m 5s\n",
            "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
            "Epoch: 222 | Time: 0m 5s\n",
            "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
            "Epoch: 223 | Time: 0m 5s\n",
            "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
            "Epoch: 224 | Time: 0m 5s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
            "Epoch: 225 | Time: 0m 5s\n",
            "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
            "Epoch: 226 | Time: 0m 5s\n",
            "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
            "Epoch: 227 | Time: 0m 5s\n",
            "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
            "Epoch: 228 | Time: 0m 5s\n",
            "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
            "Epoch: 229 | Time: 0m 5s\n",
            "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
            "Epoch: 230 | Time: 0m 5s\n",
            "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
            "Epoch: 231 | Time: 0m 5s\n",
            "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
            "Epoch: 232 | Time: 0m 5s\n",
            "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
            "Epoch: 233 | Time: 0m 5s\n",
            "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
            "Epoch: 234 | Time: 0m 5s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
            "Epoch: 235 | Time: 0m 5s\n",
            "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
            "Epoch: 236 | Time: 0m 5s\n",
            "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
            "Epoch: 237 | Time: 0m 5s\n",
            "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
            "Epoch: 238 | Time: 0m 5s\n",
            "\tTrain Loss: 0.398 | Train PPL:   1.488\n",
            "Epoch: 239 | Time: 0m 5s\n",
            "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
            "Epoch: 240 | Time: 0m 5s\n",
            "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
            "Epoch: 241 | Time: 0m 5s\n",
            "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
            "Epoch: 242 | Time: 0m 5s\n",
            "\tTrain Loss: 0.382 | Train PPL:   1.466\n",
            "Epoch: 243 | Time: 0m 5s\n",
            "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
            "Epoch: 244 | Time: 0m 5s\n",
            "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
            "Epoch: 245 | Time: 0m 5s\n",
            "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
            "Epoch: 246 | Time: 0m 5s\n",
            "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
            "Epoch: 247 | Time: 0m 5s\n",
            "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
            "Epoch: 248 | Time: 0m 5s\n",
            "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
            "Epoch: 249 | Time: 0m 5s\n",
            "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
            "Epoch: 250 | Time: 0m 5s\n",
            "\tTrain Loss: 0.359 | Train PPL:   1.431\n",
            "Epoch: 251 | Time: 0m 5s\n",
            "\tTrain Loss: 0.360 | Train PPL:   1.434\n",
            "Epoch: 252 | Time: 0m 5s\n",
            "\tTrain Loss: 0.364 | Train PPL:   1.438\n",
            "Epoch: 253 | Time: 0m 5s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
            "Epoch: 254 | Time: 0m 5s\n",
            "\tTrain Loss: 0.375 | Train PPL:   1.456\n",
            "Epoch: 255 | Time: 0m 5s\n",
            "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
            "Epoch: 256 | Time: 0m 5s\n",
            "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
            "Epoch: 257 | Time: 0m 5s\n",
            "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
            "Epoch: 258 | Time: 0m 5s\n",
            "\tTrain Loss: 0.357 | Train PPL:   1.430\n",
            "Epoch: 259 | Time: 0m 5s\n",
            "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
            "Epoch: 260 | Time: 0m 5s\n",
            "\tTrain Loss: 0.388 | Train PPL:   1.473\n",
            "Epoch: 261 | Time: 0m 5s\n",
            "\tTrain Loss: 0.393 | Train PPL:   1.481\n",
            "Epoch: 262 | Time: 0m 5s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
            "Epoch: 263 | Time: 0m 5s\n",
            "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
            "Epoch: 264 | Time: 0m 5s\n",
            "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
            "Epoch: 265 | Time: 0m 5s\n",
            "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
            "Epoch: 266 | Time: 0m 5s\n",
            "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
            "Epoch: 267 | Time: 0m 5s\n",
            "\tTrain Loss: 0.393 | Train PPL:   1.481\n",
            "Epoch: 268 | Time: 0m 5s\n",
            "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
            "Epoch: 269 | Time: 0m 5s\n",
            "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
            "Epoch: 270 | Time: 0m 5s\n",
            "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
            "Epoch: 271 | Time: 0m 5s\n",
            "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
            "Epoch: 272 | Time: 0m 5s\n",
            "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
            "Epoch: 273 | Time: 0m 5s\n",
            "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
            "Epoch: 274 | Time: 0m 5s\n",
            "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
            "Epoch: 275 | Time: 0m 5s\n",
            "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
            "Epoch: 276 | Time: 0m 5s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
            "Epoch: 277 | Time: 0m 5s\n",
            "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
            "Epoch: 278 | Time: 0m 5s\n",
            "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
            "Epoch: 279 | Time: 0m 5s\n",
            "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
            "Epoch: 280 | Time: 0m 5s\n",
            "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
            "Epoch: 281 | Time: 0m 5s\n",
            "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
            "Epoch: 282 | Time: 0m 5s\n",
            "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
            "Epoch: 283 | Time: 0m 5s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
            "Epoch: 284 | Time: 0m 5s\n",
            "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
            "Epoch: 285 | Time: 0m 5s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
            "Epoch: 286 | Time: 0m 5s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
            "Epoch: 287 | Time: 0m 5s\n",
            "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
            "Epoch: 288 | Time: 0m 5s\n",
            "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
            "Epoch: 289 | Time: 0m 5s\n",
            "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
            "Epoch: 290 | Time: 0m 5s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
            "Epoch: 291 | Time: 0m 5s\n",
            "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
            "Epoch: 292 | Time: 0m 5s\n",
            "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
            "Epoch: 293 | Time: 0m 5s\n",
            "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
            "Epoch: 294 | Time: 0m 5s\n",
            "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
            "Epoch: 295 | Time: 0m 5s\n",
            "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
            "Epoch: 296 | Time: 0m 5s\n",
            "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
            "Epoch: 297 | Time: 0m 5s\n",
            "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
            "Epoch: 298 | Time: 0m 5s\n",
            "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
            "Epoch: 299 | Time: 0m 5s\n",
            "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
            "Epoch: 300 | Time: 0m 5s\n",
            "\tTrain Loss: 0.375 | Train PPL:   1.455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b179417b4b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#valid_loss = evaluate(model, valid_iterator, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-cbed08a6d97a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VNzSiCM7VT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), \"ben-3-300.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TfokwB6VNZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "cf2d06ab-3419-4772-d7a3-702b85096482"
      },
      "source": [
        "!mv /content/ben-3-300.pt /content/drive/My\\ Drive/Ben/"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot move '/content/ben-3-300.pt' to '/content/drive/My Drive/Ben/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X761gaWdN6uz",
        "colab_type": "code",
        "outputId": "a7c19283-d017-4a0e-df35-8f774d3a2635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "source_sentence = [\"<sos>\"] + train_dataset[400].query + [\"<eos>\"]\n",
        "source_sentence = [\"<sos>\"] + [\"hello\"] + [\"<eos>\"]\n",
        "target_sentence = [\"<sos>\"] + train_dataset[400].response + [\"<eos>\"]\n",
        "print(' '.join(source_sentence))\n",
        "print(' '.join(target_sentence))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> hello <eos>\n",
            "<sos> max ? <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp0_CqDkN9qq",
        "colab_type": "code",
        "outputId": "62c2c9a5-84a0-46e7-bc11-07de67207378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "x = SRC.numericalize([source_sentence]).to(device)\n",
        "# y = TRG.numericalize([target_sentence]).to(device)\n",
        "# We actually do not have y in real world, translation should only\n",
        "# rely on source data. translate_sequences should work worse than\n",
        "# model(x, y), as it uses its own predicted tokens rather than\n",
        "# tokens from gold example (y).\n",
        "translation = model.translate_sequences(x)\n",
        "translation = translation[0].cpu().detach().numpy()\n",
        "\n",
        "print(translation)\n",
        "for x in translation[1:]:\n",
        "    word = TRG.vocab.itos[x]\n",
        "    if word == \"<eos>\":\n",
        "        break\n",
        "    print(word, end=' ')\n",
        "    #print(x)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  2 524   4   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3\n",
            "   3  16   3   3   3   3  16   3   3   3   3  16   3   3   3   3  16   3\n",
            "   4   3   3   3   3   3  16   3   3   3  16   3  16   3  16]\n",
            "hi . "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NjmCX3CvtcG",
        "colab_type": "code",
        "outputId": "fc63ea93-61c2-4274-a816-3493ef45a41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(1000,1050):\n",
        "  print(\"\\n\"+\"**\"*10)\n",
        "  source_sentence = [\"<sos>\"] + train_dataset[i].query + [\"<eos>\"]\n",
        "  target_sentence = [\"<sos>\"] + train_dataset[i].response + [\"<eos>\"]\n",
        "  print(' '.join(source_sentence))\n",
        "  print(' '.join(target_sentence))\n",
        "  x = SRC.numericalize([source_sentence]).to(device)\n",
        "  # y = TRG.numericalize([target_sentence]).to(device)\n",
        "  # We actually do not have y in real world, translation should only\n",
        "  # rely on source data. translate_sequences should work worse than\n",
        "  # model(x, y), as it uses its own predicted tokens rather than\n",
        "  # tokens from gold example (y).\n",
        "  translation = model.translate_sequences(x)\n",
        "  translation = translation[0].cpu().detach().numpy()\n",
        "  for x in translation[1:]:\n",
        "      word = TRG.vocab.itos[x]\n",
        "      if word == \"<eos>\":\n",
        "          break\n",
        "      print(word, end=' ')\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********************\n",
            "<sos> hello ? <eos>\n",
            "<sos> hello . <eos>\n",
            "in here ! \n",
            "********************\n",
            "<sos> what ? <eos>\n",
            "<sos> this -- <eos>\n",
            "nothing . \n",
            "********************\n",
            "<sos> really ? <eos>\n",
            "<sos> definitely . <eos>\n",
            "really . \n",
            "********************\n",
            "<sos> psych major ? <eos>\n",
            "<sos> english lit . <eos>\n",
            "english lit . \n",
            "********************\n",
            "<sos> sorry ... <eos>\n",
            "<sos> wow . <eos>\n",
            "wow . \n",
            "********************\n",
            "<sos> fifty percent . <eos>\n",
            "<sos> twenty . <eos>\n",
            "twenty . \n",
            "********************\n",
            "<sos> twenty . <eos>\n",
            "<sos> forty . <eos>\n",
            "forty . \n",
            "********************\n",
            "<sos> no . <eos>\n",
            "<sos> no ? <eos>\n",
            "no ? \n",
            "********************\n",
            "<sos> got it ! <eos>\n",
            "<sos> got what ? <eos>\n",
            "got what ? \n",
            "********************\n",
            "<sos> jonathan ? <eos>\n",
            "<sos> yes ? <eos>\n",
            "yes ? \n",
            "********************\n",
            "<sos> what ? <eos>\n",
            "<sos> what ? <eos>\n",
            "nothing . \n",
            "********************\n",
            "<sos> yes ! <eos>\n",
            "<sos> doctor heller ? <eos>\n",
            "general schmuck ? \n",
            "********************\n",
            "<sos> doctor heller ? <eos>\n",
            "<sos> yes ! <eos>\n",
            "yes ! \n",
            "********************\n",
            "<sos> who ? <eos>\n",
            "<sos> roy . <eos>\n",
            "the killer . \n",
            "********************\n",
            "<sos> or talk ? <eos>\n",
            "<sos> not tonight . <eos>\n",
            "not tonight . \n",
            "********************\n",
            "<sos> hi . <eos>\n",
            "<sos> alone tonight ? <eos>\n",
            "hi . \n",
            "********************\n",
            "<sos> alone tonight ? <eos>\n",
            "<sos> every night . <eos>\n",
            "every night ? \n",
            "********************\n",
            "<sos> date ? <eos>\n",
            "<sos> yeah . <eos>\n",
            "yeah . \n",
            "********************\n",
            "<sos> what ? <eos>\n",
            "<sos> bye . <eos>\n",
            "the girl . \n",
            "********************\n",
            "<sos> bye . <eos>\n",
            "<sos> monica ! <eos>\n",
            "bye . \n",
            "********************\n",
            "<sos> cover me ! <eos>\n",
            "<sos> with what ? <eos>\n",
            "with what ? \n",
            "********************\n",
            "<sos> the obliterators ! <eos>\n",
            "<sos> the eradicators ! <eos>\n",
            "the <unk> ! \n",
            "********************\n",
            "<sos> or batman -- <eos>\n",
            "<sos> or both . <eos>\n",
            "or both . \n",
            "********************\n",
            "<sos> oh no . <eos>\n",
            "<sos> great timing ! <eos>\n",
            "what ? \n",
            "********************\n",
            "<sos> through there ! <eos>\n",
            "<sos> right . <eos>\n",
            "right . \n",
            "********************\n",
            "<sos> lisette . <eos>\n",
            "<sos> only lisette ? <eos>\n",
            "only lisette ? \n",
            "********************\n",
            "<sos> no ... <eos>\n",
            "<sos> junot ? <eos>\n",
            "please ... \n",
            "********************\n",
            "<sos> five minutes . <eos>\n",
            "<sos> yes ! ! <eos>\n",
            "yes ! \n",
            "********************\n",
            "<sos> yes ! ! <eos>\n",
            "<sos> five minutes . <eos>\n",
            "five minutes . \n",
            "********************\n",
            "<sos> five minutes . <eos>\n",
            "<sos> goodbye . <eos>\n",
            "goodbye . \n",
            "********************\n",
            "<sos> separate bedrooms ? <eos>\n",
            "<sos> yes . <eos>\n",
            "yes . \n",
            "********************\n",
            "<sos> i walked . <eos>\n",
            "<sos> you walked ? <eos>\n",
            "you walked ? \n",
            "********************\n",
            "<sos> delivery ! <eos>\n",
            "<sos> hold on . <eos>\n",
            "hold on . \n",
            "********************\n",
            "<sos> rancho cucamonga ? <eos>\n",
            "<sos> yeah . <eos>\n",
            "yeah . \n",
            "********************\n",
            "<sos> amtrack ? <eos>\n",
            "<sos> what ? <eos>\n",
            "what ? \n",
            "********************\n",
            "<sos> somebody close ? <eos>\n",
            "<sos> what ? <eos>\n",
            "what ? \n",
            "********************\n",
            "<sos> i ... <eos>\n",
            "<sos> yes ? <eos>\n",
            "yes ? \n",
            "********************\n",
            "<sos> no . <eos>\n",
            "<sos> who did ? <eos>\n",
            "no ? \n",
            "********************\n",
            "<sos> ready ? <eos>\n",
            "<sos> nods . <eos>\n",
            "ready . \n",
            "********************\n",
            "<sos> meet irene . <eos>\n",
            "<sos> hi . <eos>\n",
            "hi . \n",
            "********************\n",
            "<sos> where ? <eos>\n",
            "<sos> right there . <eos>\n",
            "the cemetery ... \n",
            "********************\n",
            "<sos> yeah . <eos>\n",
            "<sos> do it ! <eos>\n",
            "yeah ? \n",
            "********************\n",
            "<sos> come back . <eos>\n",
            "<sos> yeah . <eos>\n",
            "yeah . \n",
            "********************\n",
            "<sos> any trouble ? <eos>\n",
            "<sos> no . <eos>\n",
            "no . \n",
            "********************\n",
            "<sos> uncle birdie ! <eos>\n",
            "<sos> do n't ! <eos>\n",
            "do n't ! \n",
            "********************\n",
            "<sos> he lied ! <eos>\n",
            "<sos> tricked us ! <eos>\n",
            "tricked us ! \n",
            "********************\n",
            "<sos> satan . <eos>\n",
            "<sos> ah . <eos>\n",
            "ah . \n",
            "********************\n",
            "<sos> john ... <eos>\n",
            "<sos> sshhh ... <eos>\n",
            "sshhh ... \n",
            "********************\n",
            "<sos> dead . <eos>\n",
            "<sos> dead . <eos>\n",
            "dead ? \n",
            "********************\n",
            "<sos> miz cooper ! <eos>\n",
            "<sos> what ? <eos>\n",
            "what ? "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}