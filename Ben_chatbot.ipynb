{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ben_chatbot.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/just-practice-deep/blob/master/Ben_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuwcHEtHLoJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm51ZP8QLsmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za5C9cGRLvIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7uWTZcoMeXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
        "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzWZr90jOrWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data import TabularDataset, interleave_keys\n",
        "\n",
        "train_dataset = TabularDataset(path=\"./formatted_movie_lines.txt\", format=\"CSV\",\n",
        "                               fields=[(\"query\", SRC),(\"response\", TRG)],\n",
        "                               csv_reader_params={\"delimiter\":'\\t'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIsQ-_c9zHJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_dataset, min_freq=2)\n",
        "TRG.build_vocab(train_dataset, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4gwC5BWTlHU",
        "colab_type": "code",
        "outputId": "13d1e983-3ebd-46e3-c8d6-7c1258bd5f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221282"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SydmR2II11_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuRQi9ugMj5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ7esCVR8xp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "005c9383-aa35-47d3-b64f-615f6266a9e3"
      },
      "source": [
        "from torchtext.data import Dataset\n",
        "\n",
        "def my_filter_pred(example, limited_word = 16):\n",
        "  if(len(example.query) <= limited_word and len(example.response) <= limited_word):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "my_train_dataset = Dataset(examples = train_dataset.examples,\n",
        "               fields=[(\"query\", SRC),(\"response\", TRG)],\n",
        "               filter_pred = my_filter_pred)\n",
        "\n",
        "print(\"len of this my_train_dataset is {}\".format(len(my_train_dataset)))"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of this my_train_dataset is 120314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iKzyx1HMlij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "train_iterator = BucketIterator(my_train_dataset,\n",
        "     batch_size=BATCH_SIZE,\n",
        "     sort_key=lambda x: interleave_keys(len(x.query), len(x.response)),\n",
        "     sort = True,\n",
        "     device=device,\n",
        "     shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsikYpBEZ-jq",
        "colab_type": "code",
        "outputId": "afe0e2d2-8917-4295-86dc-1cbafb905fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(list(iter(train_iterator)))"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "470"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxBLv7pBaLio",
        "colab_type": "code",
        "outputId": "d22788da-128c-42d3-9c3d-3f1be01d4855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for batch in iter(train_iterator):\n",
        "  print(batch.query.shape)"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([256, 5])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 4])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 14])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 17])\n",
            "torch.Size([256, 5])\n",
            "torch.Size([256, 6])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 7])\n",
            "torch.Size([256, 8])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 9])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256, 11])\n",
            "torch.Size([256, 12])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256, 15])\n",
            "torch.Size([256, 16])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([256, 18])\n",
            "torch.Size([250, 18])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrSqNwgkMoxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, encoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.encoder_layer = encoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([encoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
        "        \n",
        "        src = self.do((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuY02fB8Mqlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        src = self.ln(src + self.do(self.sa(src, src, src, src_mask)))\n",
        "        \n",
        "        src = self.ln(src + self.do(self.pf(src)))\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYdW9KvgMuaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \n",
        "        bsz = query.shape[0]\n",
        "        \n",
        "        #query = key = value [batch size, sent len, hid dim]\n",
        "                \n",
        "        Q = self.w_q(query)\n",
        "        K = self.w_k(key)\n",
        "        V = self.w_v(value)\n",
        "        \n",
        "        #Q, K, V = [batch size, sent len, hid dim]\n",
        "        \n",
        "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q, K, V = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = self.do(torch.softmax(energy, dim=-1))\n",
        "        \n",
        "        #attention = [batch size, n heads, sent len, sent len]\n",
        "        \n",
        "        x = torch.matmul(attention, V)\n",
        "        \n",
        "        #x = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, sent len, n heads, hid dim // n heads]\n",
        "        \n",
        "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
        "        \n",
        "        #x = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zZ34q5MMwip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.pf_dim = pf_dim\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        x = self.do(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, sent len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMeo6hh4My4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, decoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.decoder_layer = decoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([decoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch_size, trg sent len]\n",
        "        #src = [batch_size, src sent len]\n",
        "        #trg_mask = [batch size, trg sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "        \n",
        "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(self.device)\n",
        "                \n",
        "        trg = self.do((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #trg = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "            \n",
        "        return self.fc(trg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8sg-1xQM034",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.ea = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg sent len, hid dim]\n",
        "        #src = [batch size, src sent len, hid dim]\n",
        "        #trg_mask = [batch size, trg sent len]\n",
        "        #src_mask = [batch size, src sent len]\n",
        "                \n",
        "        trg = self.ln(trg + self.do(self.sa(trg, trg, trg, trg_mask)))\n",
        "                \n",
        "        trg = self.ln(trg + self.do(self.ea(trg, src, src, src_mask)))\n",
        "        \n",
        "        trg = self.ln(trg + self.do(self.pf(trg)))\n",
        "        \n",
        "        return trg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4cAKNzM3gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, sos_idx, pad_idx, device, maxlen=50):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sos_idx = sos_idx\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "    def make_masks(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "        \n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src sent len]\n",
        "        #trg_pad_mask = [batch size, 1, trg sent len, 1]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
        "                        \n",
        "        #trg_sub_mask = [trg sent len, trg sent len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg sent len, trg sent len]\n",
        "        \n",
        "        return src_mask, trg_mask\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src sent len]\n",
        "        #trg = [batch size, trg sent len]\n",
        "                \n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src sent len, hid dim]\n",
        "                \n",
        "        out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #out = [batch size, trg sent len, output dim]\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def translate_sequences(self, src):\n",
        "        #src = [batch size, src sent len]\n",
        "        \n",
        "        batch_size, src_len = src.shape\n",
        "        trg = src.new_full((batch_size, 1), self.sos_idx)\n",
        "        #trg = [batch size, 1]\n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src sent len, hid dim]\n",
        "        \n",
        "        translation_step = 0\n",
        "        while translation_step < self.maxlen:\n",
        "            out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "            # out - [batch size, trg sent len, output dim]\n",
        "            out = torch.argmax(out[:, -1], dim=1) # batch size\n",
        "            out = out.unsqueeze(1) # batch size, 1\n",
        "            trg = torch.cat((trg, out), dim=1)\n",
        "            # trg - [batch size, trg sent len]\n",
        "            src_mask, trg_mask = self.make_masks(src, trg)\n",
        "            translation_step += 1\n",
        "        return trg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSA5dd91M4X4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = len(SRC.vocab)\n",
        "hid_dim = 512\n",
        "n_layers = 4\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.2\n",
        "\n",
        "enc = Encoder(input_dim, hid_dim, n_layers, n_heads, pf_dim, EncoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q0ndCVuM71R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dim = len(TRG.vocab)\n",
        "hid_dim = 512\n",
        "n_layers = 4\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.2\n",
        "\n",
        "dec = Decoder(output_dim, hid_dim, n_layers, n_heads, pf_dim, DecoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0E40UH2M9tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = SRC.vocab.stoi['<pad>']\n",
        "sos_idx = SRC.vocab.stoi['<sos>']\n",
        "\n",
        "model = Seq2Seq(enc, dec, sos_idx, pad_idx, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw_x-s0QM_X_",
        "colab_type": "code",
        "outputId": "9156aaf3-9032-450d-e3de-b5789dc4d3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 72,986,775 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDy7Zri9NBBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVCU3TO0NDAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8lurD8ANE2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOET3uYNGob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mntX38RPNI0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.query\n",
        "        trg = batch.response\n",
        "        \n",
        "        #if(i%100==1):\n",
        "        #  print(\"src shape {}->batch {}: {}\".format(src.shape,i, loss))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg sent len - 1, output dim]\n",
        "        #trg = [batch size, trg sent len]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg sent len - 1, output dim]\n",
        "        #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACddO2tRNK-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg sent len - 1, output dim]\n",
        "            #trg = [batch size, trg sent len]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg sent len - 1, output dim]\n",
        "            #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0tWkAHGNNSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THqEDT0fNPVa",
        "colab_type": "code",
        "outputId": "6a2a0c0b-d920-4e0e-9a07-b45b4276ab3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "N_EPOCHS = 150\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    #print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 3m 51s\n",
            "\tTrain Loss: 4.132 | Train PPL:  62.290\n",
            "Epoch: 02 | Time: 3m 51s\n",
            "\tTrain Loss: 3.953 | Train PPL:  52.093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-247-93f9247d37fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#valid_loss = evaluate(model, valid_iterator, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-146-cbed08a6d97a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VNzSiCM7VT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), \"ben-5.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leIpRGknkrSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "4082f799-a6ff-4230-fd51-cbe084017ba6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TfokwB6VNZB",
        "colab_type": "code",
        "outputId": "32ebe830-cc8f-41ba-84fe-96111fcbb06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!mv /content/ben-5.pt /content/drive/My\\ Drive/Ben/"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot move '/content/ben-5.pt' to '/content/drive/My Drive/Ben/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X761gaWdN6uz",
        "colab_type": "code",
        "outputId": "32fe1683-a6f0-4844-fbb3-9f088b9ccbf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "source_sentence = [\"<sos>\"] + my_train_dataset[450].query + [\"<eos>\"]\n",
        "#source_sentence = [\"<sos>\"] + [\"not\",\"bad\"] + [\"<eos>\"]\n",
        "target_sentence = [\"<sos>\"] + my_train_dataset[450].response + [\"<eos>\"]\n",
        "print(' '.join(source_sentence))\n",
        "print(' '.join(target_sentence))"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> but , david . <eos>\n",
            "<sos> i was n't hallucinating . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp0_CqDkN9qq",
        "colab_type": "code",
        "outputId": "5e4af397-a96b-4750-f351-4cace36fbc3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = SRC.numericalize([source_sentence]).to(device)\n",
        "# y = TRG.numericalize([target_sentence]).to(device)\n",
        "# We actually do not have y in real world, translation should only\n",
        "# rely on source data. translate_sequences should work worse than\n",
        "# model(x, y), as it uses its own predicted tokens rather than\n",
        "# tokens from gold example (y).\n",
        "translation = model.translate_sequences(x)\n",
        "translation = translation[0].cpu().detach().numpy()\n",
        "\n",
        "for x in translation[1:]:\n",
        "    word = TRG.vocab.itos[x]\n",
        "    if word == \"<eos>\":\n",
        "        break\n",
        "    print(word, end=' ')\n",
        "    #print(x)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "get it ? "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NjmCX3CvtcG",
        "colab_type": "code",
        "outputId": "c153d796-9f4f-44dd-e88d-b0b0e8b78e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(10200,10250):\n",
        "  print(\"\\n\"+\"**\"*10)\n",
        "  source_sentence = [\"<sos>\"] + my_train_dataset[i].query + [\"<eos>\"]\n",
        "  target_sentence = [\"<sos>\"] + my_train_dataset[i].response + [\"<eos>\"]\n",
        "  print(' '.join(source_sentence))\n",
        "  print(' '.join(target_sentence))\n",
        "  x = SRC.numericalize([source_sentence]).to(device)\n",
        "  # y = TRG.numericalize([target_sentence]).to(device)\n",
        "  # We actually do not have y in real world, translation should only\n",
        "  # rely on source data. translate_sequences should work worse than\n",
        "  # model(x, y), as it uses its own predicted tokens rather than\n",
        "  # tokens from gold example (y).\n",
        "  translation = model.translate_sequences(x)\n",
        "  translation = translation[0].cpu().detach().numpy()\n",
        "  for x in translation[1:]:\n",
        "      word = TRG.vocab.itos[x]\n",
        "      if word == \"<eos>\":\n",
        "          break\n",
        "      print(word, end=' ')\n"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********************\n",
            "<sos> ... i ... everyone has been very kind to me . <eos>\n",
            "<sos> of course . how long did you and mr. treves prepare for this interview ? <eos>\n",
            "i 'm not right .   i 'm not . \n",
            "********************\n",
            "<sos> it was a great pleasure to meet you , mr. merrick . <eos>\n",
            "<sos> i am very pleased to meet you . <eos>\n",
            "i 'm going to be . \n",
            "********************\n",
            "<sos> i am very pleased to meet you . <eos>\n",
            "<sos> i hope we can talk together again sometime . good day . <eos>\n",
            "i 'm going to have to have to have to be a little . \n",
            "********************\n",
            "<sos> how long has this man been here ? <eos>\n",
            "<sos> three quarters of an hour . <eos>\n",
            "i do n't know . \n",
            "********************\n",
            "<sos> abominable things these machines . one ca n't reason with them . <eos>\n",
            "<sos> what a mess . <eos>\n",
            "i do n't know . \n",
            "********************\n",
            "<sos> i say freddie , what are you about ? <eos>\n",
            "<sos> oh nothing ... nothing of any great importance . <eos>\n",
            "i 'm not .   i 'm going to the way . \n",
            "********************\n",
            "<sos> certainly , if you insist . you must have quite a find there . <eos>\n",
            "<sos> i do n't know what i 've got . <eos>\n",
            "i 'm going to be right . \n",
            "********************\n",
            "<sos> i do n't know what i 've got . <eos>\n",
            "<sos> nothing of any importance , eh ? <eos>\n",
            "i 'm not .   you ? \n",
            "********************\n",
            "<sos> pardonnez - moi , i can not tell vat is \" like me \" . <eos>\n",
            "<sos> an angel is like you , kate , and you are like an angel . <eos>\n",
            "i 'm going to be . \n",
            "********************\n",
            "<sos> an angel is like you , kate , and you are like an angel . <eos>\n",
            "<sos> o bon dieu ! les langues des hommes sont pleines de tramperies . <eos>\n",
            "i 'm gon na have a little . \n",
            "********************\n",
            "<sos> oh , no . <eos>\n",
            "<sos> i knew you 'd understand . here . <eos>\n",
            "i 'm going to go . \n",
            "********************\n",
            "<sos> she 's very pretty , your mother . <eos>\n",
            "<sos> yes . <eos>\n",
            "i 'm afraid .   i do n't have a little . \n",
            "********************\n",
            "<sos> mr. treves says that you are in the theatre . do you live there ? <eos>\n",
            "<sos> oh no , mr. merrick . i just work there . <eos>\n",
            "i 'm not going to be . \n",
            "********************\n",
            "<sos> oh no , mr. merrick . i just work there . <eos>\n",
            "<sos> well , even to work there would be wonderful . is it beautiful ? <eos>\n",
            "i 'll do it . \n",
            "********************\n",
            "<sos> well , even to work there would be wonderful . is it beautiful ? <eos>\n",
            "<sos> you 've never been ? <eos>\n",
            "i do n't know . \n",
            "********************\n",
            "<sos> you 've never been ? <eos>\n",
            "<sos> alas , no . <eos>\n",
            "i do n't know .   i 'm going to you . \n",
            "********************\n",
            "<sos> have you read it ? <eos>\n",
            "<sos> no , but i certainly shall . <eos>\n",
            "i 'm going to be . \n",
            "********************\n",
            "<sos> have not saints lips , and holy palmers too ? <eos>\n",
            "<sos> ay , pilgrim , lips that they must use in prayer . <eos>\n",
            "i 'm not .   not right . \n",
            "********************\n",
            "<sos> why , mr. merrick , you 're not an elephant man at all ... <eos>\n",
            "<sos> oh no ? <eos>\n",
            "i do n't know . \n",
            "********************\n",
            "<sos> oh no ? <eos>\n",
            "<sos> oh no ... no ... you 're a romeo . <eos>\n",
            "i do n't know you know you know . \n",
            "********************\n",
            "<sos> how awful for john . <eos>\n",
            "<sos> and yet , not once have any of us heard him complain . <eos>\n",
            "i do n't know the money . \n",
            "********************\n",
            "<sos> and yet , not once have any of us heard him complain . <eos>\n",
            "<sos> is he ... dying then ? <eos>\n",
            "i 'm going to be . \n",
            "********************\n",
            "<sos> well , it 's all quite ... i 've never heard ... it 's quite ... <eos>\n",
            "<sos> yes . <eos>\n",
            "i 'm not going to be . \n",
            "********************\n",
            "<sos> you alright ? <eos>\n",
            "<sos> y - y - yes-- <eos>\n",
            "i 'm not better .   i 'm not a little . \n",
            "********************\n",
            "<sos> y - y - yes-- <eos>\n",
            "<sos> want to come out ? <eos>\n",
            "i 'm not .   i 'm not . \n",
            "********************\n",
            "<sos> want to come out ? <eos>\n",
            "<sos> you 're english . <eos>\n",
            "i 'm not a little .   i 'm going to do n't know . \n",
            "********************\n",
            "<sos> you 're english . <eos>\n",
            "<sos> of course ! you want out ? <eos>\n",
            "i 've got a little . \n",
            "********************\n",
            "<sos> of course ! you want out ? <eos>\n",
            "<sos> yes . <eos>\n",
            "i 'm going to be . \n",
            "********************\n",
            "<sos> yes . <eos>\n",
            "<sos> wo n't be a moment . <eos>\n",
            "i do n't know . \n",
            "********************\n",
            "<sos> oh no , my friend ... <eos>\n",
            "<sos> say hello to london for me . i miss her . <eos>\n",
            "i 'm not supposed , i 'm not a little . \n",
            "********************\n",
            "<sos> say hello to london for me . i miss her . <eos>\n",
            "<sos> oh , yes . <eos>\n",
            "i 'm not supposed to be . \n",
            "********************\n",
            "<sos> feeling better now , mr. merrick ? <eos>\n",
            "<sos> yes . <eos>\n",
            "i 'm not right .   i 'm not going to be , i 'm a minute . \n",
            "********************\n",
            "<sos> good morning , mr. merrick . <eos>\n",
            "<sos> good morning . <eos>\n",
            "i 'm going to be a good . \n",
            "********************\n",
            "<sos> yes . <eos>\n",
            "<sos> but it 's so good , i mean ... it 's so very good . <eos>\n",
            "i 'm not gon na be right . \n",
            "********************\n",
            "<sos> but it 's so good , i mean ... it 's so very good . <eos>\n",
            "<sos> thank you ... very much . <eos>\n",
            "i 'm not .   i 'm not a good , i 'm not . \n",
            "********************\n",
            "<sos> thank you ... very much . <eos>\n",
            "<sos> where did you get this box ? <eos>\n",
            "i 'm going to you know . \n",
            "********************\n",
            "<sos> what 's this ? <eos>\n",
            "<sos> the main spire . <eos>\n",
            "i do n't know .   i do n't know . \n",
            "********************\n",
            "<sos> i 'll have to find some more . <eos>\n",
            "<sos> yes ... well , good day , mr. merrick . <eos>\n",
            "i 'm not going to be . \n",
            "********************\n",
            "<sos> yyyy ... yyye ... yyyess . <eos>\n",
            "<sos> yes john ! <eos>\n",
            "i 'm not .   i 'm not right , i 'm not right . \n",
            "********************\n",
            "<sos> ... yyes <eos>\n",
            "<sos> yyyess . <eos>\n",
            "i 'm not a 's not . \n",
            "********************\n",
            "<sos> yyyess . <eos>\n",
            "<sos> yyess . <eos>\n",
            "i 'm not get out . \n",
            "********************\n",
            "<sos> yyess . <eos>\n",
            "<sos> that 's much better . i could understand that \" yes \" . <eos>\n",
            "i 'm going to get to be , i 'm not . \n",
            "********************\n",
            "<sos> that 's much better . i could understand that \" yes \" . <eos>\n",
            "<sos> yes ! <eos>\n",
            "i 'll be right . \n",
            "********************\n",
            "<sos> yes . <eos>\n",
            "<sos> excellent ! now , say ... \" hello \" <eos>\n",
            "i 'm not .   i 'm not a good .   i 'm not , i 'm sorry . \n",
            "********************\n",
            "<sos> excellent ! now , say ... \" hello \" <eos>\n",
            "<sos> hello ... <eos>\n",
            "i 'm going to see the truth . \n",
            "********************\n",
            "<sos> hello ... <eos>\n",
            "<sos> my name is ... <eos>\n",
            "i 'm a look to the <unk> . \n",
            "********************\n",
            "<sos> my name is ... <eos>\n",
            "<sos> my ... name is ... <eos>\n",
            "i 'm not .   i 'm going to be <unk> . \n",
            "********************\n",
            "<sos> my ... name is ... <eos>\n",
            "<sos> john merrick . <eos>\n",
            "i 'm going to get in the <unk> .   you . \n",
            "********************\n",
            "<sos> john merrick . <eos>\n",
            "<sos> john ... merrick <eos>\n",
            "i 'm going to do . \n",
            "********************\n",
            "<sos> john ... merrick <eos>\n",
            "<sos> say \" merrick \" . <eos>\n",
            "i do n't know . "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}