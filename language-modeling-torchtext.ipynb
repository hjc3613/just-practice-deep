{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled70.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/just-practice-deep/blob/master/language-modeling-torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1rIbe1pprXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext import data\n",
        "import spacy\n",
        "\n",
        "from spacy.symbols import ORTH\n",
        "my_tok = spacy.load('en')\n",
        "\n",
        "def spacy_tok(x):\n",
        "  return [tok.text for tok in my_tok.tokenizer(x)]\n",
        "\n",
        "TEXT = data.Field(lower=True, tokenize=spacy_tok)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQmM-lUPqJHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_tok.tokenizer.add_special_case(\"don't\", [{ORTH: \"do\"},{ORTH: \"n't\"}])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctLszg5EqjUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "\n",
        "train, valid, test = WikiText2.splits(TEXT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ3bt8vTq4Yx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e419bba3-0bd5-46da-87a3-23dc959ececb"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1HvwWqrALw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train, vectors=\"glove.6B.200d\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfsyGNKwrFKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_size=32,\n",
        "    bptt_len=30, # this is where we specify the sequence length\n",
        "    device=torch.device(\"cuda\"),\n",
        "    repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Piqz5Ssb2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = next(iter(train_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-KiacSqsj-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8a1f9695-a41c-4d5f-fdf6-e1682098f74a"
      },
      "source": [
        "b"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 32]\n",
              "\t[.text]:[torch.cuda.LongTensor of size 30x32 (GPU 0)]\n",
              "\t[.target]:[torch.cuda.LongTensor of size 30x32 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3F_1aMissSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9d8e325-71f7-4986-d110-a0c9cc4d9130"
      },
      "source": [
        "vars(b).keys()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['batch_size', 'dataset', 'fields', 'text', 'target'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW3wDURvs2lL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a499f81-1f5f-4e5b-d834-07b4abee1e6c"
      },
      "source": [
        "b.text.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EvYT9Gjtr_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "14b13974-aefe-4fbf-e63e-d99ff6fa6364"
      },
      "source": [
        "b.text[:5,:3]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   12,  1934, 20015],\n",
              "        [   13,    10,    30],\n",
              "        [   12,    32,     2],\n",
              "        [   15,   472, 10782],\n",
              "        [ 3875,    22,  3276]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30INhS42tTcy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "a2de107a-4d04-4999-a255-25e00d6c5066"
      },
      "source": [
        "b.target[:5, :3]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   13,    10,    30],\n",
              "        [   12,    32,     2],\n",
              "        [   15,   472, 10782],\n",
              "        [ 3875,    22,  3276],\n",
              "        [ 3895,   323,     6]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtuLstGitK8L",
        "colab_type": "text"
      },
      "source": [
        "**Training!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKmcwSUktPXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable as V\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, ntoken, ninp, nhid, nlayers, bsz, dropout=0.5, tie_weights=True):\n",
        "    super(RNNModel, self).__init__()\n",
        "    self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.encoder = nn.Embedding(ntoken, ninp)\n",
        "    self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout = dropout)\n",
        "    self.decoder = nn.Linear(nhid, ntoken)\n",
        "    self.hidden = self.init_hidden(bsz)\n",
        "  \n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.fill_(0)\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, input):\n",
        "    emb = self.drop(self.encoder(input))\n",
        "    output, self.hidden = self.rnn(emb, self.hidden)\n",
        "    output = self.drop(output)\n",
        "    decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "    return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
        "\n",
        "  def init_hidden(self, bsz):\n",
        "    weight = next(self.parameters()).data\n",
        "    return (V(weight.new(self.nlayers, bsz, self.nhid).zero_().cuda()),\n",
        "                V(weight.new(self.nlayers, bsz, self.nhid).zero_()).cuda())\n",
        "  \n",
        "  def reset_history(self):\n",
        "        self.hidden = tuple(V(v.data) for v in self.hidden)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqvakcLxSTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "1e6bc84e-2a78-4e2b-ba2e-cb50d091c50e"
      },
      "source": [
        "weight_matrix = TEXT.vocab.vectors\n",
        "BATCH_SIZE = 32\n",
        "model = RNNModel(weight_matrix.size(0), weight_matrix.size(1), 200, 1, BATCH_SIZE)\n",
        "model.encoder.weight.data.copy_(weight_matrix)\n",
        "model.cuda()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (encoder): Embedding(28870, 200)\n",
              "  (rnn): LSTM(200, 200, dropout=0.5)\n",
              "  (decoder): Linear(in_features=200, out_features=28870, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh6_sXDzyAjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.7, 0.99))\n",
        "n_tokens = weight_matrix.size(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hryU7EKsyOsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm \n",
        "def train_epoch(epoch):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(train_iter):\n",
        "    # reset the hidden state or else the model will try to backpropagate to the\n",
        "    # beginning of the dataset, requiring lots of time and a lot of memory\n",
        "         model.reset_history()\n",
        " \n",
        "    optimizer.zero_grad()\n",
        " \n",
        "    text, targets = batch.text, batch.target\n",
        "    prediction = model(text)\n",
        "    # pytorch currently only supports cross entropy loss for inputs of 2 or 4 dimensions.\n",
        "    # we therefore flatten the predictions out across the batch axis so that it becomes\n",
        "    # shape (batch_size * sequence_length, n_tokens)\n",
        "    # in accordance to this, we reshape the targets to be\n",
        "    # shape (batch_size * sequence_length)\n",
        "    loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
        "    loss.backward()\n",
        " \n",
        "    optimizer.step()\n",
        " \n",
        "    epoch_loss += loss.data * prediction.size(0) * prediction.size(1)\n",
        " \n",
        "    epoch_loss /= len(train.examples[0].text)\n",
        " \n",
        "    # monitor the loss\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    for batch in valid_iter:\n",
        "        model.reset_history()\n",
        "        text, targets = batch.text, batch.target\n",
        "        prediction = model(text)\n",
        "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
        "        val_loss += loss.data * text.size(0)\n",
        "    val_loss /= len(valid.examples[0].text)\n",
        " \n",
        "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWzdR4VXzJT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "15aec474-a21d-4ba6-ee5b-67e227e26b88"
      },
      "source": [
        "n_epochs = 3\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train_epoch(epoch)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2330/2330 [00:00<00:00, 4618.57it/s]\n",
            "  0%|          | 0/2330 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training Loss: 0.0037, Validation Loss: 0.3201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2330/2330 [00:00<00:00, 4513.43it/s]\n",
            "  0%|          | 0/2330 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2, Training Loss: 0.0037, Validation Loss: 0.3192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2330/2330 [00:00<00:00, 4657.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3, Training Loss: 0.0036, Validation Loss: 0.3182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5cJlXZ_6JJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def word_ids_to_sentence(id_tensor, vocab, join=None):\n",
        "  if isinstance(id_tensor, torch.LongTensor):\n",
        "    ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
        "  elif isinstance(id_tensor, np.ndarray):\n",
        "        ids = id_tensor.transpose().reshape(-1)\n",
        "  batch = [vocab.itos[ind] for ind in ids]\n",
        "  if join is None:\n",
        "        return batch\n",
        "  else:\n",
        "        return join.join(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atz7yZtU7F1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7ae6b7c2-55a4-4a0b-abc7-3bac1f5e611b"
      },
      "source": [
        "arrs = model(b.text).cpu().data.numpy()\n",
        "word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the the the allegation gel running stamps 52nd anticyclone clerical clerical clerical nogić baritone jewelry pace the pole > solbakken solbakken patch of the of the the the gel the the the the the the the the the the the the the the dismisses > solbakken baritone the responsibilities the the the owasco clerical clerical and clerical the the the the the the 237 carrier pole convoys solbakken celebrates duchovny the the clerical the the the the the the the kurdish averaged averaged the the the the the the the for 52nd = clerical = = = = clerical clerical clerical nogić nogić allegation = = = clerical baritone shrimp = = = = clerical clerical clerical duchovny and the the the the the the the clerical the the the the the forehead > solbakken lbw the the clerical clerical the the clerical the clerical clerical clerical of clerical the the the the the the the the the the of the the the the the clerical clerical clerical stretched the the the the the the the the the the 1691 the the the the unk > solbakken hunt the the the the the the the clerical the the the the the increasingly the the the the the the the the the the the the and and clerical and the the the the the the the clerical clerical nogić allegation interstellar shilling mycelia allegation allegation clerical clerical nogić tempo the mycelia increasingly the and alisandros lbw of the the the the the the the the the the baritone baritone the the the the the the baritone and carrier the the the the the the the the clerical the clerical the the the clerical clerical the stoneman the batches the assume assume increasingly climbs the the the the the the the the the the the the the the the the the the the the the > solbakken duchovny the apiece cardiac sherman lbw unk > solbakken there the the the the the of clerical clerical clerical california = = clerical clerical clerical clerical clerical = = = clerical clerical clerical the punch the the the the the the the the the and the the the the the the the stockton the clerical clerical chairman the the the the the the the the the the and the the the the the the the baritone 52nd clerical clerical clerical clerical extant the the clerical the the the the the the the the the variations the pole > solbakken unk solbakken solbakken horned hunt the the the the the the the the the the the the the the the extant the the the the the the the epiphany the the the the the the the clerical the the the the the the the the the the subject jurists gamecocks the the the the the the the the the the cantrell cantrell jurists stoneman the the the the the the the the the the the the the the the the the the the the the and of the solders the the the the fined the the the baritone assume baritone the dismisses the the the duchovny the the 258 the the the the the > solbakken solbakken the the buckhorn the clerical calculations calculations the the the the the the the the the the the the the the of the the the the the the stamps the annapolis the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the shrimp 52nd = = clerical clerical clerical 258 the widacki climbs the of the the the the the the the the the the the the maian the the the of the clerical the the and clerical and the the thiệu the the the the and the the the the the the fined the clerical the relentlessly unk > solbakken hunt the the the the the 258 routes duchovny the the the the the the the the the the the the the the the the the the the the the windass the the the the the the the the the the the the the the the variations squirrels clerical baritone the the the the the the the the filling convoys solbakken the the the the the the allegation = = recoilless clerical gangs for 52nd = = clerical clerical clerical duchovny the the the the clerical and and the the the the the of the the the the the the the devolved nogić raphinae 90th the clerical the clerical the the the the the the the the the the the the the the the clerical nogić nogić allegation = = baritone baritone nape clerical 52nd 52nd = = clerical clerical clerical nintendo grasslands increasingly the the the the baritone through shang climbing jesus duchovny the the the the the the the the the of kickstarter the the gaudium the the the the the the the the the the and the worst the the the the the the the the of the the the the the the the the the the the the of of the the the 258 climbs the the the the the convoys solbakken clerical clerical clerical the clerical the the the the the the clerical the the the the the the the the the the the kidnapping kidnapping the the particular the the the baritone cover baritone the particular the astonishing baritone clerical duchovny x. danzig > solbakken duchovny the solbakken solbakken solbakken duchovny unk > solbakken solbakken the calculations calculations the the the the the the the the the the the the the the the the the the the the the clerical nogić allegation = = spines'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}