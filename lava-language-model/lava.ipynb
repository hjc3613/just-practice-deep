{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled82.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/just-practice-deep/blob/master/lava-language-model/lava.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9fV3BFigTBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWooCQBjgyw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM(nn.Module):\n",
        "  \n",
        "  def __init__(self, hid_size, vocab_size, n_head, n_layers, max_len, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "    \n",
        "    self.hid_size = hid_size\n",
        "    self.max_len = max_len\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, hid_size)\n",
        "\n",
        "    self.position_enc = nn.Embedding(self.max_len, self.hid_size)\n",
        "    self.position_enc.weight.data = self.position_encoding_init(self.max_len, self.hid_size)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.hid_size])).to(device)\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(self.hid_size)\n",
        "    self.decoder_layer = nn.TransformerDecoderLayer(d_model=hid_size, nhead = n_head)\n",
        "    self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=n_layers, norm=self.layer_norm)\n",
        "    self.fc = nn.Linear(hid_size, vocab_size)\n",
        "\n",
        "    self._init_weights()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    sent_len, batch_size = x.shape[0], x.shape[1]\n",
        "    memory_mask = self.generate_complete_mask(sent_len)\n",
        "    tgt_mask = self.generate_triangular_mask(sent_len)\n",
        "    memory = torch.zeros(1, batch_size, self.hid_size, device=self.device)\n",
        "\n",
        "    temp = x\n",
        "    temp = self.embedding(temp)\n",
        "\n",
        "    pos = torch.arange(0,sent_len).unsqueeze(1).repeat(1,batch_size).to(self.device)\n",
        "    temp_pos_emb = self.position_enc(pos)\n",
        "\n",
        "    temp = temp * self.scale + temp_pos_emb\n",
        "    temp = self.decoder(temp, memory, tgt_mask=tgt_mask)\n",
        "    temp = self.fc(temp)\n",
        "    return temp\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "  def generate_triangular_mask(self, size):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
        "        return\n",
        "        \n",
        "  def generate_complete_mask(self, size):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = torch.empty(size, size).to(device)\n",
        "        mask.fill_(float('-inf'))\n",
        "        return mask\n",
        "\n",
        "  def generate_sequence(self, src):\n",
        "    #src = [sent_len]\n",
        "    src = src.unsqueeze(1)\n",
        "    #src = [sent_len, 1]\n",
        "    generate_step = 0\n",
        "    while generate_step < 10:\n",
        "      out = self.forward(src)\n",
        "      #out = [sent_len + 1, 1, vocab_size]\n",
        "      out = torch.argmax(out[-1, :], dim=1) # [1]\n",
        "      out = out.unsqueeze(0) #[1,1]\n",
        "      src = torch.cat((src, out), dim=0)\n",
        "      generate_step += 1\n",
        "    src = src.squeeze(1)\n",
        "    return src\n",
        "  \n",
        "  def position_encoding_init(self, n_position, d_pos_vec):\n",
        "    ''' Init the sinusoid position encoding table '''\n",
        "\n",
        "    # keep dim 0 for padding token position encoding zero vector\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
        "    temp = torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
        "    temp = temp.to(self.device)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epv5ydUUg7IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext import data\n",
        "import spacy\n",
        " \n",
        "my_tok = spacy.load('en')\n",
        " \n",
        "def spacy_tok(x):\n",
        "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
        " \n",
        "TEXT = data.Field(lower=True, tokenize=spacy_tok)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCRNm3nog8DD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.datasets import LanguageModelingDataset\n",
        "my_dataset = LanguageModelingDataset(\"./text.txt\", TEXT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wifobTNGhlY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cdb19655-429b-4faf-8cdb-238822224266"
      },
      "source": [
        "TEXT.build_vocab(my_dataset)\n",
        "print(len(TEXT.vocab))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkwoKb-ThtM2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0d58742f-ef68-4fc9-e3f4-05fdef18af9a"
      },
      "source": [
        "train_iter = data.BPTTIterator(\n",
        "    my_dataset,\n",
        "    batch_size=1024,\n",
        "    bptt_len=11, # this is where we specify the sequence length\n",
        "    device=device,\n",
        "    repeat=False,\n",
        "    shuffle=True)\n",
        "\n",
        "print(len(train_iter))"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn95KTHDl9u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "hid_size = 16\n",
        "model = LM(hid_size, vocab_size, 4, 1, 32, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnt8Hi7omNfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ccdeb82b-3441-4484-a7e4-ef6989a99d0e"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 237,011 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pOCXNtRmZ9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af7vatKxmb4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = NoamOpt(hid_size, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWSTmDpUmdYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d66d5c4-ea62-47ec-d81e-f450cd7dfa42"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "clip = 1\n",
        "\n",
        "N_EPOCH = 200\n",
        "for epoch in range(N_EPOCH):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  for batch in train_iter:\n",
        "    optimizer.zero_grad()\n",
        "    batch_text = batch.text\n",
        "    batch_target = batch.target\n",
        "    result = model(batch_text)\n",
        "    loss = criterion(result.view(-1, result.shape[-1]), batch_target.view(-1))\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  print(\"epoch is {} loss is {}\".format(epoch, epoch_loss / len(train_iter)))"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0 loss is 2.511195659637451\n",
            "epoch is 1 loss is 2.5533971786499023\n",
            "epoch is 2 loss is 2.4878326058387756\n",
            "epoch is 3 loss is 2.54245126247406\n",
            "epoch is 4 loss is 2.5498387813568115\n",
            "epoch is 5 loss is 2.516339123249054\n",
            "epoch is 6 loss is 2.55051052570343\n",
            "epoch is 7 loss is 2.4868292212486267\n",
            "epoch is 8 loss is 2.5083754658699036\n",
            "epoch is 9 loss is 2.4708719849586487\n",
            "epoch is 10 loss is 2.4456741213798523\n",
            "epoch is 11 loss is 2.4036545157432556\n",
            "epoch is 12 loss is 2.393104612827301\n",
            "epoch is 13 loss is 2.422989308834076\n",
            "epoch is 14 loss is 2.3539364337921143\n",
            "epoch is 15 loss is 2.3010640740394592\n",
            "epoch is 16 loss is 2.325761139392853\n",
            "epoch is 17 loss is 2.292555570602417\n",
            "epoch is 18 loss is 2.2681416273117065\n",
            "epoch is 19 loss is 2.223766803741455\n",
            "epoch is 20 loss is 2.1749913096427917\n",
            "epoch is 21 loss is 2.1705713868141174\n",
            "epoch is 22 loss is 2.1531930565834045\n",
            "epoch is 23 loss is 2.0723487734794617\n",
            "epoch is 24 loss is 2.092807173728943\n",
            "epoch is 25 loss is 2.049001395702362\n",
            "epoch is 26 loss is 2.0472411513328552\n",
            "epoch is 27 loss is 1.9942663311958313\n",
            "epoch is 28 loss is 2.008012115955353\n",
            "epoch is 29 loss is 1.949736773967743\n",
            "epoch is 30 loss is 1.9125019907951355\n",
            "epoch is 31 loss is 1.9029372930526733\n",
            "epoch is 32 loss is 1.8630640506744385\n",
            "epoch is 33 loss is 1.8608306050300598\n",
            "epoch is 34 loss is 1.8155500292778015\n",
            "epoch is 35 loss is 1.770231306552887\n",
            "epoch is 36 loss is 1.7311187982559204\n",
            "epoch is 37 loss is 1.7753064632415771\n",
            "epoch is 38 loss is 1.7202935814857483\n",
            "epoch is 39 loss is 1.6846434473991394\n",
            "epoch is 40 loss is 1.6438252329826355\n",
            "epoch is 41 loss is 1.6601591110229492\n",
            "epoch is 42 loss is 1.6294230818748474\n",
            "epoch is 43 loss is 1.5840662717819214\n",
            "epoch is 44 loss is 1.5719664692878723\n",
            "epoch is 45 loss is 1.5311315059661865\n",
            "epoch is 46 loss is 1.534623384475708\n",
            "epoch is 47 loss is 1.4849621653556824\n",
            "epoch is 48 loss is 1.4990788698196411\n",
            "epoch is 49 loss is 1.4837775826454163\n",
            "epoch is 50 loss is 1.4421035647392273\n",
            "epoch is 51 loss is 1.4170172214508057\n",
            "epoch is 52 loss is 1.4214931726455688\n",
            "epoch is 53 loss is 1.3913338780403137\n",
            "epoch is 54 loss is 1.3762919902801514\n",
            "epoch is 55 loss is 1.3793123364448547\n",
            "epoch is 56 loss is 1.3337421417236328\n",
            "epoch is 57 loss is 1.3016822338104248\n",
            "epoch is 58 loss is 1.3088645339012146\n",
            "epoch is 59 loss is 1.3059816360473633\n",
            "epoch is 60 loss is 1.2630003094673157\n",
            "epoch is 61 loss is 1.2721368074417114\n",
            "epoch is 62 loss is 1.2252570986747742\n",
            "epoch is 63 loss is 1.2004383206367493\n",
            "epoch is 64 loss is 1.2064159512519836\n",
            "epoch is 65 loss is 1.1943361163139343\n",
            "epoch is 66 loss is 1.1349543333053589\n",
            "epoch is 67 loss is 1.1453084349632263\n",
            "epoch is 68 loss is 1.1290795803070068\n",
            "epoch is 69 loss is 1.1337462067604065\n",
            "epoch is 70 loss is 1.1131699085235596\n",
            "epoch is 71 loss is 1.0833188891410828\n",
            "epoch is 72 loss is 1.0818604230880737\n",
            "epoch is 73 loss is 1.0594174265861511\n",
            "epoch is 74 loss is 1.080289900302887\n",
            "epoch is 75 loss is 1.0693204402923584\n",
            "epoch is 76 loss is 1.0322182476520538\n",
            "epoch is 77 loss is 1.02822545170784\n",
            "epoch is 78 loss is 1.0405358672142029\n",
            "epoch is 79 loss is 1.01304230093956\n",
            "epoch is 80 loss is 0.9960894286632538\n",
            "epoch is 81 loss is 0.9909687936306\n",
            "epoch is 82 loss is 0.9743185937404633\n",
            "epoch is 83 loss is 0.9831711947917938\n",
            "epoch is 84 loss is 0.9757780134677887\n",
            "epoch is 85 loss is 0.9225599467754364\n",
            "epoch is 86 loss is 0.9447254240512848\n",
            "epoch is 87 loss is 0.9252505898475647\n",
            "epoch is 88 loss is 0.8985823094844818\n",
            "epoch is 89 loss is 0.916019469499588\n",
            "epoch is 90 loss is 0.8871482014656067\n",
            "epoch is 91 loss is 0.883309543132782\n",
            "epoch is 92 loss is 0.9030851125717163\n",
            "epoch is 93 loss is 0.8875266909599304\n",
            "epoch is 94 loss is 0.8592132329940796\n",
            "epoch is 95 loss is 0.8401062488555908\n",
            "epoch is 96 loss is 0.8650471270084381\n",
            "epoch is 97 loss is 0.8418791592121124\n",
            "epoch is 98 loss is 0.8230395019054413\n",
            "epoch is 99 loss is 0.8313744366168976\n",
            "epoch is 100 loss is 0.7953216731548309\n",
            "epoch is 101 loss is 0.8196728825569153\n",
            "epoch is 102 loss is 0.8076212108135223\n",
            "epoch is 103 loss is 0.788023978471756\n",
            "epoch is 104 loss is 0.7913490533828735\n",
            "epoch is 105 loss is 0.7927463054656982\n",
            "epoch is 106 loss is 0.7782215476036072\n",
            "epoch is 107 loss is 0.7506893277168274\n",
            "epoch is 108 loss is 0.7644065618515015\n",
            "epoch is 109 loss is 0.7517503798007965\n",
            "epoch is 110 loss is 0.7425976693630219\n",
            "epoch is 111 loss is 0.7433686554431915\n",
            "epoch is 112 loss is 0.7377969920635223\n",
            "epoch is 113 loss is 0.721462607383728\n",
            "epoch is 114 loss is 0.7374247610569\n",
            "epoch is 115 loss is 0.7333353757858276\n",
            "epoch is 116 loss is 0.7336692214012146\n",
            "epoch is 117 loss is 0.7146643102169037\n",
            "epoch is 118 loss is 0.6981618404388428\n",
            "epoch is 119 loss is 0.70281121134758\n",
            "epoch is 120 loss is 0.7062886655330658\n",
            "epoch is 121 loss is 0.6953229904174805\n",
            "epoch is 122 loss is 0.6889865696430206\n",
            "epoch is 123 loss is 0.6665290296077728\n",
            "epoch is 124 loss is 0.6695467233657837\n",
            "epoch is 125 loss is 0.6726694703102112\n",
            "epoch is 126 loss is 0.6949127018451691\n",
            "epoch is 127 loss is 0.6734032332897186\n",
            "epoch is 128 loss is 0.6783193945884705\n",
            "epoch is 129 loss is 0.6874130666255951\n",
            "epoch is 130 loss is 0.6708332300186157\n",
            "epoch is 131 loss is 0.6636812686920166\n",
            "epoch is 132 loss is 0.6629824638366699\n",
            "epoch is 133 loss is 0.654301255941391\n",
            "epoch is 134 loss is 0.6514755189418793\n",
            "epoch is 135 loss is 0.6569819748401642\n",
            "epoch is 136 loss is 0.6332608163356781\n",
            "epoch is 137 loss is 0.6456680595874786\n",
            "epoch is 138 loss is 0.6541255414485931\n",
            "epoch is 139 loss is 0.6447539329528809\n",
            "epoch is 140 loss is 0.6475647687911987\n",
            "epoch is 141 loss is 0.604413628578186\n",
            "epoch is 142 loss is 0.6327255666255951\n",
            "epoch is 143 loss is 0.6052995175123215\n",
            "epoch is 144 loss is 0.6110892593860626\n",
            "epoch is 145 loss is 0.6030769646167755\n",
            "epoch is 146 loss is 0.6219103634357452\n",
            "epoch is 147 loss is 0.6048606634140015\n",
            "epoch is 148 loss is 0.6283068656921387\n",
            "epoch is 149 loss is 0.5958936959505081\n",
            "epoch is 150 loss is 0.6022057682275772\n",
            "epoch is 151 loss is 0.6060878336429596\n",
            "epoch is 152 loss is 0.5980134010314941\n",
            "epoch is 153 loss is 0.5811236798763275\n",
            "epoch is 154 loss is 0.581956148147583\n",
            "epoch is 155 loss is 0.5952300876379013\n",
            "epoch is 156 loss is 0.5772404819726944\n",
            "epoch is 157 loss is 0.5875006020069122\n",
            "epoch is 158 loss is 0.5858413130044937\n",
            "epoch is 159 loss is 0.5558272451162338\n",
            "epoch is 160 loss is 0.5758854895830154\n",
            "epoch is 161 loss is 0.5681972950696945\n",
            "epoch is 162 loss is 0.5678120255470276\n",
            "epoch is 163 loss is 0.5846361368894577\n",
            "epoch is 164 loss is 0.5625709891319275\n",
            "epoch is 165 loss is 0.5690789967775345\n",
            "epoch is 166 loss is 0.5715291798114777\n",
            "epoch is 167 loss is 0.5735138654708862\n",
            "epoch is 168 loss is 0.5583305656909943\n",
            "epoch is 169 loss is 0.5599116683006287\n",
            "epoch is 170 loss is 0.5469710528850555\n",
            "epoch is 171 loss is 0.5473843812942505\n",
            "epoch is 172 loss is 0.5499135255813599\n",
            "epoch is 173 loss is 0.5606203675270081\n",
            "epoch is 174 loss is 0.5334896445274353\n",
            "epoch is 175 loss is 0.5474977791309357\n",
            "epoch is 176 loss is 0.521288275718689\n",
            "epoch is 177 loss is 0.5403924435377121\n",
            "epoch is 178 loss is 0.521227166056633\n",
            "epoch is 179 loss is 0.535780668258667\n",
            "epoch is 180 loss is 0.522053673863411\n",
            "epoch is 181 loss is 0.5239748507738113\n",
            "epoch is 182 loss is 0.5184189677238464\n",
            "epoch is 183 loss is 0.5281574130058289\n",
            "epoch is 184 loss is 0.5219393223524094\n",
            "epoch is 185 loss is 0.5216165333986282\n",
            "epoch is 186 loss is 0.5145599097013474\n",
            "epoch is 187 loss is 0.5224056094884872\n",
            "epoch is 188 loss is 0.5415851473808289\n",
            "epoch is 189 loss is 0.5267348736524582\n",
            "epoch is 190 loss is 0.5251421183347702\n",
            "epoch is 191 loss is 0.5077209770679474\n",
            "epoch is 192 loss is 0.4941856563091278\n",
            "epoch is 193 loss is 0.5090068429708481\n",
            "epoch is 194 loss is 0.5203333497047424\n",
            "epoch is 195 loss is 0.50445257127285\n",
            "epoch is 196 loss is 0.5209688544273376\n",
            "epoch is 197 loss is 0.5046953856945038\n",
            "epoch is 198 loss is 0.4906303137540817\n",
            "epoch is 199 loss is 0.49618083238601685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dna3eHXbm574",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8848750b-0017-4a63-86ed-9680be545f0e"
      },
      "source": [
        "source_sentence = [\"the\"]\n",
        "print(source_sentence)\n",
        "model.eval()\n",
        "print(' '.join(source_sentence))\n",
        "print()\n",
        "x = TEXT.numericalize([source_sentence]).to(device).squeeze(1)\n",
        "generated_sequence =model.generate_sequence(x)\n",
        "words = [TEXT.vocab.itos[word_idx] for word_idx in generated_sequence]\n",
        "print(' '.join(words))"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the']\n",
            "the\n",
            "\n",
            "the one <eos> language is like your design is the <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}